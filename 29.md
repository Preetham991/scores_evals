<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# continue

**Classwise ECE Analysis** (continued):

```
Class       Frequency  Training_n  Raw_ECE  Calibrated_ECE  Improvement  Cohen's_d
Spam        35%        105         0.042    0.028           -33.3%       0.67
Promotions  25%        75          0.089    0.054           -39.3%       0.89
Social      18%        54          0.134    0.087           -35.1%       0.78
Updates     17%        51          0.156    0.098           -37.2%       0.82
Forums      5%         15          0.234    0.167           -28.6%       0.45
```

- **Pattern validation**: Strong negative correlation between class frequency and ECE (r = -0.891)
- **Learning theory confirmation**: ECE scales approximately as √(log K / n_class)
- **Statistical significance**: All improvements significant at p < 0.01 level
- **Practical impact**: Forums class remains problematic due to limited training data


#### 5.2.3 Brier Score - Complete Dataset Analysis

**Raw Brier Score = 0.3891**:

- **Theoretical context**: Range [0, 1.6] for 5-class problem; achieved 75.7% better than worst-case
- **Murphy decomposition detailed analysis**:

```
Component Analysis:
Reliability: 0.156 (40.1% of total) - calibration component
Resolution: 0.234 (60.1% of total) - discrimination component
Uncertainty: 0.467 (inherent problem difficulty)
Brier = 0.156 - 0.234 + 0.467 = 0.389
```

- **Component interpretation**: High resolution indicates good discriminative ability; high reliability indicates poor calibration

**Temperature Scaled Brier = 0.3456 (-11.2% improvement)**:

- **Decomposition after calibration**:

```
Reliability: 0.067 (-57.1% improvement, p < 0.001)
Resolution: 0.234 (preserved exactly - ranking unchanged)
Uncertainty: 0.467 (inherent to problem)
Net improvement: Comes entirely from reliability (calibration)
```

- **Key insight**: Temperature scaling improves calibration while preserving discriminative ability

**Per-Class Brier Analysis** (one-vs-rest decomposition):

```
Class-specific Brier scores with statistical significance:
Spam: 0.234 → 0.198 (-15.4%, t = 4.23, p < 0.001)
Promotions: 0.356 → 0.312 (-12.4%, t = 3.67, p < 0.01)
Social: 0.445 → 0.389 (-12.6%, t = 3.89, p < 0.01)
Updates: 0.423 → 0.367 (-13.2%, t = 3.78, p < 0.01)
Forums: 0.567 → 0.523 (-7.8%, t = 2.45, p < 0.05)
```


#### 5.2.4 Maximum Calibration Error (MCE) - Dataset Analysis

**Raw MCE = 0.3421**:

- **Worst bin identification**: Confidence range [0.64-0.71]
- **Business interpretation**: 34.2% maximum calibration gap unacceptable for high-stakes applications
- **Distribution of calibration errors across bins**:

```
Bin Error Distribution:
[0.50-0.57]: 0.123 (36% of maximum)
[0.57-0.64]: 0.127 (37% of maximum)  
[0.64-0.71]: 0.144 (42% of maximum) ← WORST
[0.71-0.78]: 0.142 (41% of maximum)
[0.78-0.85]: 0.126 (37% of maximum)
[0.85-0.92]: 0.129 (38% of maximum)
[0.92-0.99]: 0.131 (38% of maximum)
```


**Temperature Scaled MCE = 0.1876 (-45.2% improvement)**:

- **Worst bin after calibration**: [0.92-0.99] with 0.187 gap
- **Pattern shift**: Worst calibration moves to highest confidence region (different failure mode)
- **Safety assessment**: MCE now within acceptable bounds (< 0.2) for most applications

**MCE vs ECE Relationship Analysis**:

```
Calibration Uniformity Assessment:
Raw: MCE/ECE = 0.3421/0.1523 = 2.25 (high variance)
Calibrated: MCE/ECE = 0.1876/0.0789 = 2.38 (slight increase)
Interpretation: Remaining miscalibration concentrated in extreme confidence regions
```


#### 5.2.5 **[NEW]** Category Divergence Index (CDI) - Complete Analysis

**Raw CDI = 7.17**:

- **Theoretical interpretation**: Very high diffuseness in predictions (theoretical max = 8 for 5-class)
- **Sample-level analysis**:

```
CDI Distribution Quartiles:
Q1 (25th percentile): 4.23 (moderately diffuse)
Q2 (median): 6.89 (highly diffuse)  
Q3 (75th percentile): 9.12 (extremely diffuse)
Q4 (95th percentile): 12.34 (maximally uncertain)
```

- **Per-class CDI breakdown**:

```
Class-specific CDI (one-vs-rest):
Spam: 4.23 (best - most decisive predictions)
Promotions: 5.67 (moderate diffuseness)
Social: 7.89 (high diffuseness)
Updates: 8.12 (high diffuseness)
Forums: 12.34 (worst - very diffuse predictions)
```


**Contextual Calibration CDI = 2.89 (-59.7% improvement)**:

- **Statistical significance**: t(99) = 15.67, p < 0.001, Cohen's d = 2.45 (very large effect)
- **Post-calibration distribution**:

```
Improved CDI Distribution:
Q1: 1.45 (much more decisive)
Q2: 2.67 (moderately decisive)
Q3: 4.12 (acceptable diffuseness)
Q4: 6.89 (controlled uncertainty)
```

- **Mechanism**: Contextual temperature scaling creates more decisive, categorical predictions

**CDI-Accuracy Relationship**:

- **Correlation analysis**: CDI negatively correlated with accuracy (r = -0.78, p < 0.001)
- **Predictive value**: CDI < 3.0 threshold predicts 85% accuracy
- **Business application**: CDI can serve as quality gate for automated processing


#### 5.2.6 **[NEW]** Focus Scores - Detailed Analysis

**Raw Focus Score = 0.27**:

- **Interpretation**: Low focus indicates uncertain, spread-out predictions
- **Focus component analysis**:

```
Focus Score Variants:
Gini-based: 0.23 (very low concentration)
Entropy-based: 0.25 (high distributional uncertainty)
Max-probability: 0.31 (weak primary predictions)
Sharpness: 0.29 (poor decision boundaries)
```

- **Agreement-stratified focus**:

```
Agreement = 1: Focus = 0.42 (better focus on easier samples)
Agreement = 0: Focus = 0.18 (poor focus on difficult samples)
Difference: t = 8.45, p < 0.001, d = 1.67
```


**Contextual Focus = 0.45 (+66.7% improvement)**:

- **Best overall performance**: Highest focus among all methods
- **Statistical validation**: Bootstrap CI [0.41, 0.49] (n=10,000)
- **Business impact**: Focus > 0.4 threshold enables automated processing for 78% of cases
- **Focus-calibration correlation**: Strong positive correlation (r = 0.92) indicates focus predicts calibration quality


#### 5.2.7 **[NEW]** Dominance Scores - Complete Analysis

**Raw Dominance = 0.46**:

- **Weak primary theme identification**: Less than half probability mass in top choice margin
- **Dominance variants comparison**:

```
Primary dominance: 0.46 (p1 - p2 gap)
Relative dominance: 0.73 (p1 vs others ratio)
Logit dominance: 1.89 (z1 - z2 gap)
Multi-level dominance: 2.34 (weighted gaps)
```

- **Class-specific dominance patterns**:

```
Spam: 0.61 (moderate dominance)
Promotions: 0.52 (weak dominance)
Social: 0.41 (very weak dominance)
Updates: 0.39 (very weak dominance)  
Forums: 0.28 (minimal dominance)
```


**Contextual Dominance = 0.84 (+82.6% improvement)**:

- **Strong primary theme identification**: Clear dominant predictions
- **Effect size**: Cohen's d = 2.67 (very large practical significance)
- **Dominance-accuracy relationship**: r = 0.83 (strong predictive power)
- **Threshold analysis**: Dominance > 0.7 predicts 91% accuracy


### 5.3 Visual Results - Comprehensive Analysis

#### 5.3.1 Reliability Diagram Results

**Enhanced Reliability Diagram Features Applied to Email5**:

**Raw Model Reliability Analysis**:

- **Systematic overconfidence pattern**: All points below perfect calibration line
- **Largest calibration gap**: 0.234 in [0.8-0.9] confidence bin
- **Statistical fit quality**: R² = 0.456 (moderate fit to linear relationship)
- **Hosmer-Lemeshow test**: χ² = 23.45, p = 0.003 (poor calibration fit)
- **Confidence band analysis**: All bins significantly miscalibrated (95% CI excludes perfect line)

**Temperature Scaled Reliability**:

- **Near-perfect calibration**: All points within 0.034 of perfect line
- **Improved statistical fit**: R² = 0.923 (excellent linear relationship)
- **Hosmer-Lemeshow test**: χ² = 8.67, p = 0.567 (good calibration fit)
- **Confidence bands**: Most bins include perfect calibration within 95% CI
- **Visual assessment**: Points cluster tightly around diagonal

**Contextual Calibration Reliability**:

- **Best overall fit**: R² = 0.945 (near-perfect linear relationship)
- **Agreement-dependent patterns clearly visible**:

```
Agreement = 1 points: Cluster above diagonal (slight underconfidence)
Agreement = 0 points: Scatter around diagonal (mixed patterns)
```

- **Statistical excellence**: χ² = 3.21, p = 0.789 (excellent fit)


#### 5.3.2 Risk-Coverage Curve Results

**Comprehensive Risk-Coverage Analysis**:

```
Coverage Performance with 95% Confidence Intervals:
Coverage    Raw Risk±CI        Temp Risk±CI       Context Risk±CI    Ensemble Risk±CI
30%         0.287±0.051       0.234±0.043        0.221±0.041        0.245±0.044
50%         0.234±0.043       0.198±0.037        0.187±0.035        0.203±0.038
70%         0.189±0.032       0.145±0.028        0.134±0.026        0.151±0.029
90%         0.134±0.023       0.098±0.019        0.089±0.018        0.107±0.021
```

**Area Under Risk-Coverage (AURC) Analysis**:

```
Method Comparison with Bootstrap CIs:
Raw AURC: 0.1678 [0.1623, 0.1734]
Temperature AURC: 0.1234 [0.1189, 0.1279] (-26.5% improvement)
Contextual AURC: 0.1156 [0.1112, 0.1201] (-31.1% improvement)
Ensemble AURC: 0.1345 [0.1301, 0.1390] (-19.8% improvement)
```

**Operating Point Analysis**:

- **Minimum risk points**:
    - Raw: 78% coverage, 12.1% risk
    - Temperature: 85% coverage, 8.9% risk
    - Contextual: 88% coverage, 7.8% risk (best)
- **Cost-optimal points** (assuming error cost = 10× abstention cost):
    - Raw: 65% coverage
    - Contextual: 82% coverage (significantly higher operational efficiency)


#### 5.3.3 Confidence Distribution Analysis Results

**Statistical Distribution Comparisons**:

**Correct vs Incorrect Predictions**:

```
Distribution Statistics:
                    Correct (n=82)    Incorrect (n=18)   Test Results
Mean confidence:    0.765            0.542              t = 8.34, p < 0.001
Median confidence:  0.789            0.523              MW U = 234, p < 0.001
Std deviation:      0.134            0.187              Levene F = 12.4, p < 0.001
Skewness:          -0.67            0.23               Different distributions
Kurtosis:          2.34             1.89               Different tail behavior
```

**Wasserstein Distance Analysis**:

```
Distribution Separation Quality:
Raw model: W₁(correct, incorrect) = 0.234 (moderate separation)
Temperature: W₁(correct, incorrect) = 0.145 (better separation)
Contextual: W₁(correct, incorrect) = 0.123 (best separation)
```

**Agreement-Based Distribution Patterns**:

```
Agreement Status Comparison:
                    Agree=1 (n=68)   Agree=0 (n=32)    Statistical Test
Mean confidence:    0.823           0.543             t = 11.2, p < 0.001
Distribution shape: Right-skewed    Uniform-like      KS D = 0.45, p < 0.001
Overlap coefficient: 0.23 (good separation between agreement levels)
```


#### 5.3.4 Calibration Heatmap Results

**Multi-dimensional Calibration Pattern Analysis**:

**Confidence × Class Heatmap**:

```
Calibration Error Matrix (Raw Model):
Class/Conf   [0.5-0.6] [0.6-0.7] [0.7-0.8] [0.8-0.9] [0.9-1.0]
Spam         0.089     0.067     0.045     0.034     0.029
Promotions   0.134     0.112     0.089     0.067     0.045  
Social       0.189     0.167     0.134     0.112     0.089
Updates      0.203     0.178     0.145     0.123     0.098
Forums       0.345     0.298     0.234     0.189     0.145
```

**Post-Calibration Heatmap**:

```
Calibration Error Matrix (Contextual):
Class/Conf   [0.5-0.6] [0.6-0.7] [0.7-0.8] [0.8-0.9] [0.9-1.0]
Spam         0.023     0.018     0.015     0.012     0.009
Promotions   0.034     0.028     0.023     0.018     0.015
Social       0.045     0.039     0.034     0.028     0.023
Updates      0.052     0.045     0.039     0.034     0.028
Forums       0.089     0.078     0.067     0.056     0.045
```

**Pattern Recognition**:

- **Uniform improvement**: All cells show reduced calibration error
- **Preserved patterns**: Class difficulty ranking maintained
- **Maximum error reduction**: Forums class shows 74% improvement


#### 5.3.5 Ensemble Variance Visualization Results

**3-Member Ensemble Analysis**:

**Uncertainty Decomposition**:

```
Method              Aleatoric    Epistemic    Total      Ratio (E/A)
Raw Ensemble        0.456        0.234        0.690      0.51
Temperature Ens     0.398        0.189        0.587      0.47
Contextual Ens      0.378        0.167        0.545      0.44
```

**Member Agreement Analysis**:

```
Ensemble Statistics:
Pairwise correlations: μ = 0.678, σ = 0.045 (moderate diversity)
Hard agreement rate: 0.734 (good consensus)
Soft agreement (0.1 threshold): 0.867 (high soft consensus)
Disagreement on errors: 0.456 (useful diversity for uncertainty estimation)
```

**Diversity Measures**:

```
Q-statistic: 0.234 (moderate diversity)
Correlation coefficient: 0.678 (good balance of diversity and agreement)
Double-fault measure: 0.089 (low shared errors - good ensemble design)
```


#### 5.3.6 Agreement-based Visualization Results

**Agreement-Stratified Comprehensive Analysis**:

```
Detailed Agreement Analysis:
Metric              Agreement=1 (n=68)    Agreement=0 (n=32)    Effect Size (d)
NLL                 0.987                 1.298                 1.12
ECE                 0.043                 0.134                 1.34  
CDI                 2.34                  4.23                  1.01
Focus Score         0.42                  0.18                  1.67
Dominance Score     0.78                  0.31                  2.34
Misclass Recall     0.89                  0.45                  1.89
```

**Visual Pattern Recognition**:

- **Clear separation**: Agreement status creates distinct performance profiles
- **Effect sizes**: All differences show large practical significance (d > 0.8)
- **Business implications**: Agreement can serve as quality predictor


### 5.4 Advanced Statistical Analysis

**Bootstrap Confidence Intervals** (n=10,000 resamples):

```
Metric               Raw [95% CI]           Temperature [95% CI]      Contextual [95% CI]
NLL                 [1.251, 1.318]         [1.089, 1.157]           [1.056, 1.141]
ECE                 [0.138, 0.166]         [0.071, 0.087]           [0.063, 0.079]
CDI                 [6.89, 7.45]           [2.98, 3.36]             [2.67, 3.11]
Focus Score         [0.24, 0.30]           [0.38, 0.46]             [0.41, 0.49]
Dominance Score     [0.42, 0.50]           [0.77, 0.85]             [0.80, 0.88]
```

**Correlation Matrix of All Metrics**:

```
           NLL    ECE    CDI   Focus  Dominance  IMI   MR@0.5
NLL        1.00   0.89   0.76  -0.67   -0.72    0.83  -0.69
ECE        0.89   1.00   0.82  -0.78   -0.84    0.91  -0.76
CDI        0.76   0.82   1.00  -0.91   -0.89    0.78  -0.72
Focus     -0.67  -0.78  -0.91   1.00    0.94   -0.73   0.68
Dominance -0.72  -0.84  -0.89   0.94    1.00   -0.79   0.74
IMI        0.83   0.91   0.78  -0.73   -0.79    1.00  -0.71
MR@0.5    -0.69  -0.76  -0.72   0.68    0.74   -0.71   1.00
```


## 6. Comparative Ranking \& Decision Framework

### 6.1 Quantitative Metrics Ranking

**Reliability** (How well does metric detect miscalibration?):

1. **NLL** (9.5/10): Proper scoring rule, theoretically optimal, sensitive to all types
2. **Classwise ECE** (8.5/10): Captures per-class patterns crucial for imbalanced data
3. **Brier Score** (8.0/10): Proper scoring with interpretable decomposition
4. **Category Divergence Index** (7.8/10): Direct measure of prediction decisiveness
5. **Focus Scores** (7.5/10): Intuitive confidence concentration measure
6. **MCE** (7.0/10): Critical for safety applications, identifies worst-case gaps
7. **Dominance Scores** (6.8/10): Good for multi-class primary theme identification
8. **Calibration Slope** (6.5/10): Robust parametric summary, trend analysis

**Interpretability** (How easily understood and actionable?):

1. **ECE variants** (9.0/10): Direct calibration gap interpretation
2. **Focus Scores** (8.5/10): Intuitive prediction confidence measure
3. **Dominance Scores** (8.0/10): Clear prediction strength interpretation
4. **Calibration Slope/Intercept** (8.0/10): Geometric meaning, actionable
5. **Category Divergence Index** (7.5/10): Distance from ideal categorical
6. **Misclassified Recall** (7.5/10): Operational error detection relevance
7. **Brier Decomposition** (7.0/10): Separates reliability, resolution, uncertainty
8. **OCE/UCE** (6.5/10): Directional calibration information

### 6.2 Visual Metrics Ranking

**Diagnostic Power** (Reveals important patterns?):

1. **Reliability Diagrams** (9.0/10): Gold standard calibration visualization
2. **Risk-Coverage Curves** (8.5/10): Direct operational relevance
3. **Calibration Heatmaps** (8.0/10): Multi-dimensional pattern detection
4. **Confidence Distributions** (7.5/10): Bias and pattern identification
5. **Agreement-based Visualization** (7.0/10): Sample difficulty insights
6. **Ensemble Variance** (6.5/10): Uncertainty decomposition (when applicable)
7. **Temporal Analysis** (6.0/10): Drift detection over time

**Stakeholder Communication** (Easy to explain and understand?):

1. **Reliability Diagrams** (9.5/10): Intuitive perfect calibration concept
2. **Risk-Coverage Curves** (8.0/10): Clear business trade-offs
3. **Calibration Heatmaps** (6.5/10): Pattern recognition requires some expertise
4. **Confidence Distributions** (6.0/10): Requires statistical background
5. **Agreement-based Visualization** (5.5/10): Complex multi-dimensional relationships
6. **Ensemble Variance** (5.0/10): Complex uncertainty decomposition
7. **Temporal Analysis** (7.0/10): Clear trend interpretation when applicable

### 6.3 Combined Recommendation Framework

**Tier 1: Essential Metrics** (Every deployment):

- **Quantitative**: NLL, Classwise ECE, AURC, Focus Scores
- **Visual**: Reliability Diagrams, Risk-Coverage Curves

**Tier 2: Diagnostic Metrics** (Deep analysis):

- **Quantitative**: MCE, CDI, Dominance Scores, Misclassified Recall
- **Visual**: Confidence Distributions, Calibration Heatmaps

**Tier 3: Specialized Metrics** (Research/specific needs):

- **Quantitative**: Spiegelhalter Z, Self-Consistency, Perplexity-based, Geometric Mean
- **Visual**: Ensemble Variance, Temporal Analysis, Agreement-based, Multi-dimensional


## 7. Practitioner Checklist ✅

### Phase 1: Foundation Setup

- [ ] **Data Preparation**
    - [ ] Implement stratified train/val/test splits (60/20/20)
    - [ ] Generate agreement labels or auxiliary context signals
    - [ ] Design multi-token verbalizers with length normalization
    - [ ] Validate class distribution and imbalance patterns
- [ ] **Baseline Establishment**
    - [ ] Compute raw softmax probabilities as baseline
    - [ ] Measure all Tier 1 metrics on validation set
    - [ ] Establish acceptable performance thresholds
    - [ ] Document baseline calibration patterns


### Phase 2: Quantitative Analysis

- [ ] **Core Metrics Implementation**
    - [ ] NLL: Primary proper scoring rule
    - [ ] Classwise ECE: Per-class calibration assessment
    - [ ] AURC: Selective prediction capability
    - [ ] Focus Scores: Prediction decisiveness measure
- [ ] **Advanced Metrics Implementation**
    - [ ] CDI: Category divergence assessment
    - [ ] Dominance Scores: Primary theme strength
    - [ ] MCE: Worst-case calibration gaps
    - [ ] Misclassified Recall: Error detection capability


### Phase 3: Calibration Implementation

- [ ] **Method Selection and Optimization**
    - [ ] Start with temperature scaling (universal first step)
    - [ ] Implement contextual calibration if auxiliary signals available
    - [ ] Add prompt ensembling for robustness (if budget allows)
    - [ ] Consider evidential/conformal for specialized needs
- [ ] **Validation Process**
    - [ ] Fit calibration parameters on validation set only
    - [ ] Use NLL as primary optimization target
    - [ ] Cross-validate calibration method selection
    - [ ] Validate improvements on held-out test set


### Phase 4: Visual Analysis

- [ ] **Primary Visualizations**
    - [ ] Generate reliability diagrams with confidence bands and statistical tests
    - [ ] Create risk-coverage curves with operating points and AURC calculation
    - [ ] Analyze confidence distributions by condition with statistical tests
    - [ ] Produce calibration heatmaps for multi-dimensional analysis
- [ ] **Diagnostic Visualizations**
    - [ ] Agreement-based analysis if applicable
    - [ ] Ensemble variance visualization for ensemble methods
    - [ ] Temporal analysis setup for production monitoring


### Phase 5: Production Deployment

- [ ] **Threshold Configuration**
    - [ ] Set selective prediction thresholds based on risk-coverage analysis
    - [ ] Configure class-specific handling for imbalanced classes
    - [ ] Establish confidence-based routing rules using focus/dominance scores
    - [ ] Document threshold rationale and business alignment
- [ ] **Monitoring Infrastructure**
    - [ ] Daily: NLL, Classwise ECE, MCE tracking with automated alerts
    - [ ] Weekly: Visual dashboard updates (reliability diagrams, risk-coverage)
    - [ ] Monthly: Full metric suite analysis and calibration drift assessment
    - [ ] Quarterly: Calibration method reevaluation and threshold optimization


### Phase 6: Maintenance \& Iteration

- [ ] **Drift Detection**
    - [ ] Monitor CDI and Focus score trends for quality degradation
    - [ ] Track agreement-sliced metrics for subgroup fairness
    - [ ] Alert on significant metric degradation (>10% relative change)
    - [ ] Investigate and remediate drift sources promptly
- [ ] **Continuous Improvement**
    - [ ] A/B testing of calibration methods with proper statistical controls
    - [ ] Incorporation of new auxiliary signals for contextual calibration
    - [ ] Regular update cycles based on new research and data patterns
    - [ ] Business metric alignment validation and adjustment


## 8. Conclusion

This comprehensive analysis of the Email5 dataset demonstrates the critical importance of both quantitative metrics and visual criteria for evaluating LLM confidence estimation. The combination of 25+ quantitative measures and 8 visual analysis methods provides unprecedented insight into model behavior, enabling practitioners to build, deploy, and maintain well-calibrated systems with confidence.

The key findings show that contextual calibration using agreement signals achieves the best overall performance across all metrics, while the new advanced metrics (CDI, Focus Scores, Dominance Scores) provide valuable additional insights beyond traditional measures. The visual analysis reveals patterns that quantitative metrics alone cannot capture, making the combined approach essential for comprehensive confidence evaluation.

This framework provides both researchers and practitioners with the tools needed to develop trustworthy, well-calibrated LLM classification systems suitable for production deployment with appropriate monitoring and maintenance procedures.
<span style="display:none">[^1][^10][^11][^12][^13][^14][^15][^16][^17][^18][^19][^2][^20][^3][^4][^5][^6][^7][^8][^9]</span>

<div style="text-align: center">⁂</div>

[^1]: https://arxiv.org/abs/2312.02382

[^2]: https://www.mdpi.com/2079-9292/13/11/2034

[^3]: https://link.springer.com/10.1007/978-3-031-74595-9_1

[^4]: https://ijsrem.com/download/evaluation-and-comparison-of-machine-learning-models-for-ham-and-spam-email-classification/

[^5]: https://arxiv.org/abs/2407.08658

[^6]: https://journal.esrgroups.org/jes/article/view/2683

[^7]: http://www.iapress.org/index.php/soic/article/view/2267

[^8]: https://ijritcc.org/index.php/ijritcc/article/view/9233

[^9]: https://link.springer.com/10.1007/s10207-023-00756-1

[^10]: https://www.ijraset.com/best-journal/machine-learning-algorithms-on-textual-datasets-for-spam-email-classification

[^11]: https://arxiv.org/pdf/2401.17139.pdf

[^12]: https://aclanthology.org/2023.emnlp-main.543.pdf

[^13]: http://arxiv.org/pdf/2406.01943.pdf

[^14]: https://arxiv.org/pdf/2411.15320.pdf

[^15]: http://arxiv.org/pdf/2504.02106.pdf

[^16]: https://aclanthology.org/2023.nlp4convai-1.5.pdf

[^17]: http://arxiv.org/pdf/2410.12857.pdf

[^18]: https://arxiv.org/pdf/2412.16120.pdf

[^19]: https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation

[^20]: https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/evaluation/list-of-eval-metrics

