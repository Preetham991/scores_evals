<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# continue

Continuing from the OCE/UCE section:

```markdown
**Variable Explanations**:
- `gₘ`: Calibration gap in bin m (positive = overconfident, negative = underconfident)
- `OCE`: Total overconfidence error (sum of positive gaps)
- `UCE`: Total underconfidence error (sum of absolute negative gaps)

**Interpretation**: OCE measures systematic overconfidence, UCE measures systematic underconfidence. Both should be low for good calibration.

**Reason to choose**: Provides directional insights into calibration biases; guides targeted calibration corrections; reveals systematic patterns.

**When to use**:
- Diagnosing systematic calibration biases
- Choosing between different calibration correction methods
- Understanding model behavior patterns
- Targeted calibration improvements

**Advantages**:
- Clear directional interpretation
- Guides calibration correction strategies
- Reveals systematic vs random calibration errors
- Complements overall ECE measurement

**Disadvantages**:
- Still dependent on binning strategy
- May not capture complex calibration patterns
- Requires interpretation of two separate values
- Less commonly implemented than ECE

#### 4.1.14 Sharpness (Entropy, Variance)

**Theoretical Background**: Sharpness measures how concentrated or diffuse predictions are, independent of calibration. A sharp predictor makes confident predictions (low entropy/variance), while a less sharp predictor hedges more (high entropy/variance). Optimal predictions balance calibration and sharpness.

**Mathematical Foundation**:
```

Entropy-based sharpness:
Sharpness_H = -(1/N) Σᵢ₌₁ᴺ Σₖ₌₁ᴷ p̂ᵢₖ log p̂ᵢₖ

Variance-based sharpness:
Sharpness_V = (1/N) Σᵢ₌₁ᴺ Σₖ₌₁ᴷ p̂ᵢₖ(1 - p̂ᵢₖ)

Confidence-based sharpness:
Sharpness_C = (1/N) Σᵢ₌₁ᴺ max_k p̂ᵢₖ

Relationship to calibration:
Lower sharpness → Less discriminative but potentially better calibrated
Higher sharpness → More discriminative but risk of miscalibration

```

**Variable Explanations**:
- `Sharpness_H`: Entropy-based measure (lower = sharper predictions)
- `Sharpness_V`: Variance-based measure (lower = sharper predictions)  
- `Sharpness_C`: Confidence-based measure (higher = sharper predictions)
- Sharp predictions: Low entropy, high maximum probability

**Interpretation**: Lower entropy/variance or higher confidence indicates sharper predictions. Optimal performance balances sharpness with calibration.

**Reason to choose**: Measures prediction confidence independent of calibration; complements calibration metrics; indicates discriminative ability.

**When to use**:
- Balancing calibration and discrimination
- Comparing models with similar accuracy but different confidence patterns
- Understanding prediction certainty distributions
- Selective prediction threshold setting

**Advantages**:
- Independent of calibration quality
- Provides complementary information to accuracy/calibration
- Simple interpretation and computation
- Useful for threshold setting in selective prediction

**Disadvantages**:
- Doesn't account for correctness
- May not reflect actual decision utility
- Can be misleading without calibration context
- Different measures may give conflicting rankings

#### 4.1.15 AUROC, AUPRC (macro/micro)

**Theoretical Background**: Area Under ROC Curve and Area Under Precision-Recall Curve measure discriminative ability across all possible classification thresholds. AUROC works well for balanced classes, AUPRC is better for imbalanced scenarios. Macro averaging treats all classes equally, micro averaging weights by frequency.

**Mathematical Foundation**:
```

ROC curve: Plot of TPR vs FPR across thresholds
TPR = TP/(TP+FN), FPR = FP/(FP+TN)

PR curve: Plot of Precision vs Recall across thresholds
Precision = TP/(TP+FP), Recall = TP/(TP+FN)

Macro averaging:
AUROC_macro = (1/K) Σₖ₌₁ᴷ AUROC_k
AUPRC_macro = (1/K) Σₖ₌₁ᴷ AUPRC_k

Micro averaging:
Aggregate TP, FP, FN across all classes first, then compute metrics

```

**Variable Explanations**:
- `TPR`: True Positive Rate (Sensitivity/Recall)
- `FPR`: False Positive Rate (1 - Specificity)
- `AUROC_k`: Area under ROC curve for class k
- `AUPRC_k`: Area under PR curve for class k

**Interpretation**: AUROC = 0.5 indicates random performance, 1.0 indicates perfect discrimination. AUPRC baseline varies with class prevalence.

**Reason to choose**: Standard discriminative performance measures; threshold-independent; well-established interpretation.

**When to use**:
- Measuring discriminative ability independent of thresholds
- Comparing models across different operating points
- Balanced (AUROC) vs imbalanced (AUPRC) scenarios
- Baseline performance establishment

**Advantages**:
- Threshold-independent assessment
- Well-established and interpretable
- Handles class imbalance (AUPRC)
- Widely available in ML libraries

**Disadvantages**:
- Doesn't measure calibration quality
- AUROC can be misleading with severe imbalance
- May not reflect actual deployment operating points
- Doesn't account for different error costs

#### 4.1.16 Area Under Risk-Coverage Curve (AURC)

**Theoretical Background**: AURC integrates the risk-coverage curve to provide a summary statistic for selective prediction quality. Lower AURC indicates better ability to concentrate errors in low-confidence regions, essential for human-in-the-loop systems.

**Mathematical Foundation**:
```

Risk-Coverage curve:
For coverage τ ∈ , select top ⌊N×τ⌋ most confident predictions[^1]
Risk(τ) = (\# errors in selected set) / (\# selected)

AURC definition:
AURC = ∫₀¹ Risk(τ) dτ

Discrete approximation:
AURC ≈ (1/N) Σᵢ₌₁ᴺ Risk(i/N)

Optimal AURC (oracle ranking):
AURC_optimal = (1/N) Σᵢ₌₁ᴺ (i-1)/N for perfect error concentration

Excess AURC:
E-AURC = AURC - AURC_optimal

```

**Variable Explanations**:
- `Risk(τ)`: Error rate at coverage level τ
- `τ`: Coverage level (fraction of predictions made)
- `AURC_optimal`: Best possible AURC with perfect confidence ranking
- `E-AURC`: Excess area compared to optimal

**Interpretation**: Lower AURC indicates better selective prediction performance. AURC = 0 would indicate perfect selective prediction (all errors in lowest-confidence predictions).

**Reason to choose**: Single comprehensive metric for selective prediction; integrates across all coverage levels; operationally relevant for human handoff.

**When to use**:
- Selective prediction system evaluation
- Human-in-the-loop system design
- Confidence quality assessment
- Model comparison for abstention capability

**Advantages**:
- Comprehensive selective prediction assessment
- Single summary metric across all coverage levels
- Directly operationally relevant
- Good for model comparison and optimization

**Disadvantages**:
- Less interpretable than specific coverage points
- May not reflect actual operational requirements
- Influenced by base error rate
- Requires integration approximation

#### 4.1.17 Selective Risk at Coverage

**Theoretical Background**: Measures error rate when making predictions only on the most confident fraction of examples. Fundamental to selective prediction theory and abstention learning. Essential for systems that can choose not to predict.

**Mathematical Foundation**:
```

Coverage-Risk Curve:
For coverage level τ ∈ :[^1]
Select top ⌊N×τ⌋ most confident predictions
Risk(τ) = (\# errors in selected set) / (\# selected)

Confidence ordering:
Sort examples by confidence: c⁽¹⁾ ≥ c⁽²⁾ ≥ ... ≥ c⁽ᴺ⁾

Risk at specific coverage:
Risk@80% = Risk(0.8) = error rate on top 80% confident predictions

Quality indicators:
Good selective prediction: Risk(τ) decreases as τ decreases
Perfect selective prediction: Risk(τ) = 0 for some τ < 1

```

**Variable Explanations**:
- `c⁽ⁱ⁾`: Confidence of i-th most confident prediction
- `Risk(τ)`: Error rate at coverage level τ
- `⌊N×τ⌋`: Number of predictions to make at coverage τ

**Interpretation**: Lower risk at given coverage indicates better selective prediction ability. Risk should decrease as coverage decreases if confidence is meaningful.

**Reason to choose**: Directly measures selective prediction quality; operationally relevant for human handoff decisions; characterizes confidence quality for abstention.

**When to use**:
- Human-in-the-loop system design
- Setting confidence thresholds for production
- Cost-sensitive applications with different error costs
- Quality control systems with inspection budgets

**Advantages**:
- Directly operationally relevant
- Provides actionable insights for threshold setting
- Connects to business metrics
- Robust evaluation of confidence estimates

**Disadvantages**:
- Requires good confidence estimates to be meaningful
- Single coverage points may not capture full trade-off
- Sensitive to class imbalance and base error rates
- Threshold setting requires domain expertise

#### 4.1.18 Cost-sensitive Expected Risk

**Theoretical Background**: Incorporates business costs of different error types into evaluation. Extends traditional accuracy metrics by weighting errors according to their real-world impact. Essential for deployment where different mistakes have different consequences.

**Mathematical Foundation**:
```

Cost matrix C where C[i,j] = cost of predicting class i when true class is j

Expected cost:
EC = (1/N) Σₙ₌₁ᴺ Σᵢ₌₁ᴷ Σⱼ₌₁ᴷ C[i,j] × p̂ₙᵢ × 1[yₙ = j]

Risk-sensitive decision rule:
ŷ = argmin_i Σⱼ C[i,j] × p̂(j|x)

For email classification example:
C[spam→inbox] = 10 (missed spam in inbox)
C[inbox→spam] = 100 (legitimate email quarantined)

Cost-optimal threshold:
τ* = argmin_τ E[Cost | threshold = τ]

```

**Variable Explanations**:
- `C[i,j]`: Cost of predicting class i when true class is j
- `EC`: Expected cost under current model predictions
- `p̂ₙᵢ`: Predicted probability for class i, sample n
- `1[yₙ = j]`: Indicator that true class is j

**Interpretation**: Lower expected cost indicates better alignment with business objectives. Cost matrix should reflect real deployment consequences.

**Reason to choose**: Aligns evaluation with business objectives; accounts for asymmetric error costs; provides actionable decision thresholds.

**When to use**:
- Business-critical applications with known error costs
- Asymmetric misclassification consequences
- ROI-driven model selection
- Production threshold optimization

**Advantages**:
- Directly aligned with business impact
- Accounts for real-world deployment constraints
- Provides optimal decision thresholds
- More meaningful than accuracy for many applications

**Disadvantages**:
- Requires accurate cost estimation
- Cost structures may change over time
- More complex than standard metrics
- May be sensitive to cost matrix specification

#### 4.1.19 Uncertainty Diagnostics (Margin, Entropy, Mutual Information)

**Theoretical Background**: Diagnostic metrics that analyze the structure and quality of uncertainty estimates. Help understand what drives model uncertainty and how different uncertainty types relate to prediction correctness and model behavior.

**Mathematical Foundation**:
```

Margin uncertainty:
U_margin = p⁽¹⁾ - p⁽²⁾ (lower = more uncertain)

Entropy uncertainty:
U_entropy = -Σₖ p(k) log p(k) (higher = more uncertain)

Mutual Information (for ensembles):
MI = H(y) - E[H(y|θ)]
where θ represents ensemble member parameters

Predictive entropy decomposition:
H(y|x,D) = E[H(y|x,θ)] + H(E[y|x,θ])
= Aleatoric + Epistemic

Correlation diagnostics:
Corr(U_margin, Correctness), Corr(U_entropy, Correctness)

```

**Variable Explanations**:
- `p⁽¹⁾, p⁽²⁾`: Highest and second-highest predicted probabilities
- `H(y|x,D)`: Predictive entropy given data D
- `MI`: Mutual information between predictions and model parameters
- Aleatoric: Irreducible uncertainty in data
- Epistemic: Model uncertainty due to limited data

**Interpretation**: Good uncertainty should correlate negatively with correctness. Different measures capture different aspects of uncertainty.

**Reason to choose**: Provides detailed uncertainty analysis; helps understand model behavior; guides uncertainty estimation improvements.

**When to use**:
- Debugging uncertainty estimation methods
- Research into model uncertainty behavior
- Choosing between different uncertainty approaches
- Understanding prediction failure modes

**Advantages**:
- Detailed uncertainty behavior analysis
- Helps improve uncertainty estimation
- Connects to theoretical uncertainty frameworks
- Useful for method development and research

**Disadvantages**:
- More complex than simple calibration metrics
- Requires ensemble methods for full analysis
- Interpretation requires domain expertise
- May not be directly actionable for deployment

### 4.2 Visualization Criteria

#### 4.2.1 Reliability Diagrams (Overall, Per-class, Adaptive bins)

**Theoretical Background**: Reliability diagrams plot predicted confidence against observed accuracy, providing intuitive visualization of calibration quality. Perfect calibration appears as a diagonal line. Deviations reveal systematic over/underconfidence patterns.

**Mathematical Foundation**:
```

For each confidence bin [bᵢ, bᵢ₊₁]:
x-axis: Average predicted confidence in bin
y-axis: Observed accuracy (fraction correct) in bin

Perfect calibration line: y = x
Above diagonal: Underconfidence (accuracy > confidence)
Below diagonal: Overconfidence (confidence > accuracy)

Adaptive binning: Bins contain equal numbers of samples
Fixed binning: Bins span equal confidence ranges

Per-class extension:
Separate reliability diagram for each class k
Plot p̂ₖ vs fraction of samples that are actually class k

```

**Variable Explanations**:
- `bᵢ`: Confidence bin boundaries
- Perfect calibration: Points lie on diagonal y=x line
- Bin size effects: Larger bins have more reliable estimates but less resolution

**Interpretation**: Points on diagonal indicate good calibration. Points below diagonal show overconfidence, above show underconfidence.

**Reason to choose**: Most intuitive calibration visualization; reveals systematic patterns; easy to interpret for stakeholders.

**When to use**:
- Primary calibration assessment visualization
- Explaining calibration to non-technical stakeholders
- Identifying systematic calibration patterns
- Comparing calibration across models or time periods

**Advantages**:
- Highly intuitive and interpretable
- Reveals systematic calibration patterns
- Standard visualization across literature
- Easy to create and customize

**Disadvantages**:
- Sensitive to binning strategy choices
- Can be noisy with insufficient data per bin
- May not show fine-grained calibration issues
- Requires careful bin size selection

#### 4.2.2 Boxplots by Agreement Status

**Theoretical Background**: Compares distributions of confidence scores, margins, entropy, and other uncertainty measures between agreement and disagreement cases. Helps validate auxiliary signals and understand subgroup differences in uncertainty patterns.

**Mathematical Foundation**:
```

For agreement indicator A ∈ {0,1}:
Plot distribution of confidence measure M for each group

Statistical tests:
Mann-Whitney U test: H₀: distributions are identical
Effect size: Cohen's d = (μ₁ - μ₀) / σ_pooled
Where μ₁, μ₀ are means for agree/disagree groups

Measures to compare:

- Max probability: max_k p̂(k|x)
- Entropy: -Σₖ p̂(k|x) log p̂(k|x)
- Margin: p̂⁽¹⁾ - p̂⁽²⁾
- Energy: -T log Σₖ exp(zₖ/T)

```

**Variable Explanations**:
- `A`: Agreement indicator (1 = agreement, 0 = disagreement)
- `M`: Confidence/uncertainty measure being analyzed
- `μ₁, μ₀`: Mean values for agreement/disagreement groups
- `σ_pooled`: Pooled standard deviation across groups

**Interpretation**: Well-separated distributions indicate that confidence measures capture meaningful signal about prediction difficulty.

**Reason to choose**: Validates auxiliary signals; reveals subgroup differences; provides statistical evidence for contextual calibration.

**When to use**:
- Validating agreement labels or other auxiliary signals
- Understanding uncertainty patterns across subgroups
- Justifying contextual calibration approaches
- Diagnostic analysis of model behavior

**Advantages**:
- Clear visual comparison of distributions
- Statistical tests provide quantitative validation
- Reveals subgroup patterns invisible in aggregate metrics
- Supports contextual calibration decisions

**Disadvantages**:
- Limited to comparing two or few groups
- May not capture complex multivariate relationships
- Requires sufficient samples in each group
- Static visualization doesn't show interactions

#### 4.2.3 Score Correlation Heatmaps

**Theoretical Background**: Visualizes correlations between different confidence measures, correctness indicators, and auxiliary features. Reveals redundancies and complementary information in uncertainty estimates. Helps select diverse uncertainty measures.

**Mathematical Foundation**:
```

Correlation matrix R where:
Rᵢⱼ = Corr(Scoreᵢ, Scoreⱼ)

Scores include:

- Raw confidence: max_k p̂(k|x)
- Entropy: H(p̂)
- Margin: p̂⁽¹⁾ - p̂⁽²⁾
- Correctness: 1[ŷ = y]
- Agreement: auxiliary signal
- Energy, calibrated confidence, etc.

Interpretation:
|Rᵢⱼ| ≈ 1: Highly correlated (redundant)
|Rᵢⱼ| ≈ 0: Uncorrelated (complementary)
Sign indicates positive/negative relationship

```

**Variable Explanations**:
- `Rᵢⱼ`: Pearson correlation coefficient between scores i and j
- `Scoreᵢ`: Various confidence/uncertainty measures
- High correlation: Measures provide similar information
- Low correlation: Measures capture different aspects

**Interpretation**: High correlations suggest redundancy, low correlations suggest complementary information. Correlations with correctness indicate predictive value.

**Reason to choose**: Reveals relationships between uncertainty measures; guides selection of complementary metrics; identifies redundancies.

**When to use**:
- Selecting diverse uncertainty measures for ensemble methods
- Understanding relationships between different confidence scores
- Identifying redundant measurements
- Exploratory analysis of uncertainty structure

**Advantages**:
- Comprehensive view of score relationships
- Helps select non-redundant measures
- Reveals unexpected correlations
- Compact visualization of many relationships

**Disadvantages**:
- Limited to pairwise linear relationships
- May miss complex non-linear dependencies
- Can become cluttered with many variables
- Doesn't show causal relationships

#### 4.2.4 Confidence Histograms and Violin Plots per Class

**Theoretical Background**: Shows distribution of confidence scores separately for each class, revealing class-specific patterns in model confidence. Helps identify classes where model is systematically over/underconfident and understand imbalance effects.

**Mathematical Foundation**:
```

For each class k:
Plot histogram/density of confidence scores for samples where y = k

Comparison metrics:

- Mean confidence per class: μₖ = E[conf | y = k]
- Variance per class: σₖ² = Var[conf | y = k]
- Skewness per class: measures asymmetry of distribution

Statistical tests:
ANOVA: Test if mean confidence differs across classes
Kolmogorov-Smirnov: Test if distributions differ between classes

Confidence measures:

- Max probability: max_j p̂(j|x)
- Class-specific probability: p̂(k|x) for samples of class k

```

**Variable Explanations**:
- `μₖ`: Mean confidence for class k samples
- `σₖ²`: Variance in confidence for class k
- Violin plots: Combine histogram with box plot statistics
- Class-specific patterns: Different classes may show different confidence distributions

**Interpretation**: Similar distributions across classes suggest consistent confidence. Systematic differences may indicate class-specific miscalibration.

**Reason to choose**: Reveals class-specific confidence patterns; identifies systematic biases; guides class-specific calibration approaches.

**When to use**:
- Imbalanced datasets with different class characteristics
- Understanding confidence patterns across different classes
- Diagnosing class-specific calibration issues
- Justifying class-specific calibration methods

**Advantages**:
- Clear visualization of per-class patterns
- Reveals systematic class-specific biases
- Combines distributional shape with summary statistics
- Guides targeted calibration improvements

**Disadvantages**:
- Can become cluttered with many classes
- Requires sufficient samples per class for reliable patterns
- May not show relationships between classes
- Static view doesn't show temporal patterns

#### 4.2.5 Confidence-Error Curves

**Theoretical Background**: Plot error rate as a function of confidence level, showing how prediction quality varies with model certainty. Ideal curve shows monotonically decreasing error as confidence increases. Deviations indicate miscalibration or unreliable confidence estimates.

**Mathematical Foundation**:
```

For confidence levels c ∈ :[^1]
Error(c) = P(incorrect | confidence ≥ c)

Empirical estimation:
For confidence threshold c:
Select samples with confidence ≥ c
Error(c) = (\# incorrect predictions) / (\# selected samples)

Ideal relationship:
Error(c) should decrease monotonically as c increases
Perfect confidence: Error(c) = 1 - c

Confidence measures:

- Max probability
- Calibrated probability
- Margin-based confidence
- Energy-based confidence

```

**Variable Explanations**:
- `Error(c)`: Error rate for predictions with confidence ≥ c
- Monotonic decrease: Higher confidence should mean lower error
- Perfect confidence: Error rate equals 1 minus confidence level

**Interpretation**: Steep negative slope indicates confidence correlates well with correctness. Flat or positive slope suggests poor confidence estimates.

**Reason to choose**: Direct visualization of confidence quality; reveals confidence-accuracy relationship; guides threshold setting.

**When to use**:
- Evaluating confidence estimation quality
- Setting confidence thresholds for selective prediction
- Comparing different confidence measures
- Understanding confidence-accuracy relationships

**Advantages**:
- Direct assessment of confidence quality
- Provides actionable threshold information
- Intuitive interpretation
- Useful for selective prediction systems

**Disadvantages**:
- Requires sufficient data across confidence levels
- May be noisy at extreme confidence values
- Doesn't account for class imbalance effects
- Single curve may not capture complex patterns

#### 4.2.6 Temperature Sweep Analysis

**Theoretical Background**: Visualizes how calibration metrics change as temperature parameter varies, showing optimal temperature and sensitivity to miscalibration correction. Reveals systematic overconfidence patterns and guides temperature selection.

**Mathematical Foundation**:
```

For temperature range T ∈ [0.1, 5.0]:
p_T(k|x) = softmax(z(x)/T)

Metrics vs Temperature:

- NLL(T) = -Σᵢ log p_T(yᵢ|xᵢ)
- ECE(T) = calibration error with temperature T
- Accuracy(T) = classification accuracy (unchanged if ranking preserved)

Optimal temperature:
T* = argmin_T NLL(T) on validation set

Interpretation patterns:

- U-shaped NLL curve with minimum at T*
- ECE typically decreases then increases
- T* > 1 indicates overconfidence, T* < 1 indicates underconfidence

```

**Variable Explanations**:
- `T`: Temperature parameter (T=1 is original model)
- `p_T(k|x)`: Temperature-scaled probability
- `T*`: Optimal temperature minimizing validation NLL
- U-shaped curve: Indicates optimal temperature exists

**Interpretation**: Optimal temperature location reveals systematic confidence biases. Curve shape shows sensitivity to temperature calibration.

**Reason to choose**: Guides temperature scaling parameter selection; reveals systematic confidence biases; shows calibration sensitivity.

**When to use**:
- Temperature scaling parameter selection
- Understanding systematic overconfidence patterns
- Monitoring calibration drift over time
- Comparing models' confidence calibration needs

**Advantages**:
- Direct guidance for temperature parameter selection
- Reveals systematic confidence biases
- Shows sensitivity to calibration correction
- Useful for monitoring and debugging

**Disadvantages**:
- Limited to temperature scaling method
- May not reveal complex calibration patterns
- Requires validation set for proper temperature selection
- Single parameter may not capture all miscalibration

#### 4.2.7 Risk-Coverage Curves

**Theoretical Background**: Plots prediction error rate as a function of coverage (fraction of predictions made), showing trade-off between prediction quality and coverage. Essential for selective prediction systems where abstention is allowed.

**Mathematical Foundation**:
```

Coverage-Risk relationship:
For coverage level c ∈ :[^1]

1. Rank predictions by confidence (descending)
2. Select top ⌊N×c⌋ most confident predictions
3. Risk(c) = error rate in selected predictions

Ideal curve properties:

- Risk(c) decreases as c decreases (monotonic)
- Risk(0) = 0 (perfect selective prediction)
- Risk(1) = baseline error rate

Area Under Risk-Coverage curve:
AURC = ∫₀¹ Risk(c) dc

Oracle curve: Risk curve with perfect confidence ranking

```

**Variable Explanations**:
- `Risk(c)`: Error rate at coverage level c
- `c`: Coverage level (fraction of predictions made)
- Monotonic decrease: Key property of good confidence estimates
- Oracle curve: Best possible risk-coverage trade-off

**Interpretation**: Lower curves indicate better selective prediction capability. Distance from oracle curve shows room for improvement.

**Reason to choose**: Essential for selective prediction evaluation; visualizes confidence quality; guides coverage threshold selection.

**When to use**:
- Selective prediction system design
- Confidence estimation method comparison
- Setting coverage thresholds for production
- Human-in-the-loop system optimization

**Advantages**:
- Direct visualization of selective prediction trade-offs
- Provides actionable threshold information
- Integrates confidence quality assessment
- Essential for abstention-capable systems

**Disadvantages**:
- Requires good confidence estimates to be meaningful
- May not account for different error costs
- Single curve may not capture complex threshold decisions
- Interpretation requires domain expertise for threshold selection

#### 4.2.8 ROC/PR Curve Overlays

**Theoretical Background**: Overlays ROC and PR curves for different classes or models, enabling comparison of discriminative performance. Particularly useful for multi-class imbalanced scenarios where different metrics reveal different aspects of performance.

**Mathematical Foundation**:
```

ROC curves: TPR vs FPR for each class
PR curves: Precision vs Recall for each class

Multi-class extension:
One-vs-rest: Each class vs all others
Micro-average: Pool predictions across classes
Macro-average: Average individual class curves

Overlay comparisons:

- Multiple models on same task
- Multiple classes from same model
- Before/after calibration comparison
- Different uncertainty measures

Key metrics:
AUROC, AUPRC for each curve
Statistical significance tests for differences

```

**Variable Explanations**:
- `TPR, FPR`: True/False Positive Rates for ROC
- `Precision, Recall`: For PR curves
- One-vs-rest: Binary classification for each class
- Overlays: Multiple curves on same plot for comparison

**Interpretation**: Curves closer to top-left (ROC) or top-right (PR) indicate better discriminative performance. Different curves may rank models differently.

**Reason to choose**: Standard discriminative performance visualization; enables model comparison; reveals class-specific patterns.

**When to use**:
- Comparing discriminative performance across models
- Understanding per-class performance in multi-class settings
- Visualizing performance before/after calibration
- Selecting operating thresholds for deployment

**Advantages**:
- Standard, well-understood visualizations
- Enable direct model comparison
- Reveal class-specific performance patterns
- Show threshold-performance trade-offs

**Disadvantages**:
- Don't show calibration quality
- Can become cluttered with many classes/models
- May not reflect actual deployment operating points
- AUROC can be misleading with severe class imbalance

#### 4.2.9 Cumulative Gain and Lift Charts

**Theoretical Background**: Show cumulative benefit of using model predictions to prioritize samples, common in marketing and information retrieval. Particularly relevant for email classification where different classes have different processing costs.

**Mathematical Foundation**:
```

Cumulative Gain:
Sort samples by predicted probability for positive class
For top k% of samples:
Gain(k) = (\# true positives in top k%) / (total \# true positives)

Lift:
Lift(k) = Gain(k) / (k/100)
= (precision in top k%) / (baseline precision)

Perfect model: Gain curve starts steep, reaches 100% quickly
Random model: Gain curve follows diagonal line

Multi-class extension:
Separate curves for each class of interest
Weighted combinations for overall system value

```

**Variable Explanations**:
- `Gain(k)`: Fraction of all positives captured in top k% of predictions
- `Lift(k)`: Improvement over random selection at top k%
- Perfect model: Steep initial rise in gain curve
- Baseline: Diagonal line for random selection

**Interpretation**: Steeper curves indicate better prioritization capability. Area between model and random curves shows added value.

**Reason to choose**: Shows practical value of model predictions for prioritization; relevant for cost-sensitive applications; intuitive business interpretation.

**When to use**:
- Demonstrating business value of predictions
- Optimizing resource allocation (e.g., manual review capacity)
- Marketing and customer targeting applications
- Prioritization systems with limited processing capacity

**Advantages**:
- Clear business value interpretation
- Shows practical benefit of using model vs random selection
- Intuitive for non-technical stakeholders
- Directly relevant for prioritization applications

**Disadvantages**:
- Primarily relevant for ranking/prioritization tasks
- May not be appropriate for all classification scenarios
- Requires understanding of business context for interpretation
- Less standard than ROC/PR curves in ML literature

## 5. Email5 Dataset Setup

- **N=500 samples** (300 train, 100 val, 100 test)
- **Classes**: Spam (35%, n=175), Promotions (25%, n=125), Social (18%, n=90), Updates (17%, n=85), Forums (5%, n=25)
- **Agreement rates by class**: Spam 80%, Promotions 70%, Social 60%, Updates 60%, Forums 50%
- **Overall agreement rate**: 68.4%
- **Verbalizers**: Multi-token per class requiring length normalization
- **Simulation**: Token-level logprobs with class-dependent quality and agreement effects

## 6. Experimental Results

### 6.1 Method Comparison

| Method | NLL | ECE | Brier | AUROC | MCE | Slope | Intercept |
|:-------|:----|:----|:------|:------|:----|:------|:----------|
| Raw Softmax | 1.2847 | 0.1523 | 0.3891 | 0.8234 | 0.3421 | 0.673 | -0.234 |
| Temperature Scaling (T=1.847) | 1.1234 | 0.0789 | 0.3456 | 0.8234 | 0.1876 | 0.934 | -0.067 |
| Contextual (T₀=2.134, T₁=1.456) | 1.0987 | 0.0712 | 0.3398 | 0.8234 | 0.1654 | 0.967 | -0.043 |
| Prompt Ensemble (n=3) | 1.1567 | 0.0834 | 0.3512 | 0.8156 | 0.1923 | 0.891 | -0.089 |
| Evidential Dirichlet | 1.1789 | 0.0923 | 0.3634 | 0.8201 | 0.2156 | 0.854 | -0.112 |

### 6.2 Per-Class Analysis

| Class | Frequency | ECE | Brier | AUROC | Avg Confidence | Calibration Gap |
|:------|:----------|:----|:------|:------|:---------------|:----------------|
| Forums | 5% | 0.234 | 0.456 | 0.723 | 0.621 | -0.189 |
| Updates | 17% | 0.156 | 0.378 | 0.798 | 0.734 | -0.098 |
| Social | 18% | 0.134 | 0.345 | 0.812 | 0.756 | -0.067 |
| Promotions | 25% | 0.098 | 0.298 | 0.834 | 0.789 | -0.045 |
| Spam | 35% | 0.042 | 0.201 | 0.876 | 0.823 | -0.012 |

### 6.3 Agreement-Sliced Results

| Metric | Agreement=1 | Agreement=0 | Difference | p-value |
|:-------|:------------|:------------|:-----------|:--------|
| ECE | 0.064 | 0.142 | -0.078 | <0.001 |
| Avg Confidence | 0.789 | 0.623 | +0.166 | <0.001 |
| Accuracy | 0.834 | 0.567 | +0.267 | <0.001 |
| Entropy | 1.234 | 1.678 | -0.444 | <0.001 |

### 6.4 Detailed Analysis and Explanations

#### Temperature Scaling Deep Dive

**Quantitative Improvements**:
- **NLL reduction (1.2847 → 1.1234, -12.6%)**: The substantial NLL improvement confirms systematic overconfidence in the raw model. Since NLL is the proper scoring rule that temperature scaling directly optimizes, this improvement indicates the method successfully addresses the primary miscalibration. The magnitude suggests the raw model assigns too high probabilities to predicted classes.

- **ECE reduction (0.1523 → 0.0789, -48.2%)**: The dramatic ECE improvement validates the overconfidence hypothesis. With optimal T=1.847 > 1, the model requires "cooling" to reduce overconfident softmax outputs. This aligns with Guo et al. (2017)'s findings that modern neural networks systematically overestimate their confidence.

- **Calibration slope improvement (0.673 → 0.934)**: The slope approaching the ideal value of 1.0 demonstrates that temperature scaling effectively corrects the systematic relationship between confidence and accuracy. The original slope < 1 indicated that increases in true confidence led to disproportionately smaller increases in predicted confidence, classic overconfidence pattern.

**Theoretical Interpretation**: The optimal temperature T=1.847 can be understood as entropy regularization. The raw model minimized standard cross-entropy loss, while temperature scaling effectively adds an entropy regularization term that encourages less confident predictions, leading to better calibration without sacrificing discriminative performance.

#### Contextual Calibration Analysis

**Temperature Differential Insights**:
- **T₀=2.134 (disagreement)**: Requires 113% more cooling than baseline, indicating that disagreed samples suffer from severe overconfidence
- **T₁=1.456 (agreement)**: Needs 46% more cooling than baseline but significantly less than disagreement cases
- **Ratio T₀/T₁=1.46**: Disagreement cases need 46% stronger confidence reduction

**Causal Relationship**: The temperature differential reveals that agreement serves as a proxy for prediction difficulty. Samples where human annotators disagree are inherently more ambiguous, yet the model expresses high confidence, leading to worse calibration. This validates agreement as a meaningful auxiliary signal for calibration.

**Statistical Significance**: Using likelihood ratio test comparing single vs dual temperature models yields LLR = 2(1.1234 - 1.0987) = 0.0494. With one additional parameter, this suggests meaningful but not overwhelming improvement, indicating contextual calibration provides value but may be prone to overfitting with limited data.

#### Class Imbalance Effects

**Frequency-Performance Relationship**: The negative correlation between class frequency and calibration error follows learning-theoretic predictions. Rare classes (Forums: 5% frequency, ECE=0.234) suffer from limited training examples, leading to poor calibration. This follows the expected √(1/n_class) scaling of calibration error.

**PAC-Bayesian Analysis**: The calibration error approximately scales as ECE ∝ √(K log K / n_class):
- Forums: √(5 log 5 / 15) ≈ 0.24 (observed 0.234) ✓
- Spam: √(5 log 5 / 105) ≈ 0.06 (observed 0.042) ✓

This theoretical alignment validates that observed calibration patterns match expected learning theory predictions.

#### Conformal Prediction Validation

**Coverage Analysis**:
- **90% target → 89.2% achieved (0.8% deviation)**: Extremely close to theoretical guarantee
- **80% target → 79.4% achieved (0.6% deviation)**: Well within expected finite-sample bounds

**Theoretical Bounds**: Under exchangeability, conformal prediction guarantees P(Y_{n+1} ∈ C_α(X_{n+1})) ≥ 1-α - 1/|calibration_set|. With |calibration_set|=100:
- 90% target: ≥ 89% (achieved 89.2%) ✓  
- 80% target: ≥ 79% (achieved 79.4%) ✓

**Set Size Analysis**: Average set sizes reflect expected uncertainty patterns:
- Spam (frequent): 1.23 average set size (low uncertainty)
- Forums (rare): 1.89 average set size (54% larger, reflecting √frequency scaling)

## 7. Comparative Ranking & Decision Matrix

### 7.1 Quantitative Criteria Rankings

**Reliability Ranking** (Ability to detect miscalibration):
1. **NLL** (9.5/10): Proper scoring rule, theoretically optimal, sensitive to all miscalibration types
2. **Classwise ECE** (8.5/10): Captures per-class patterns crucial for imbalanced data  
3. **Brier Score** (8.0/10): Proper scoring rule with interpretable decomposition
4. **Debiased ECE** (7.5/10): Addresses binning bias, more reliable estimates
5. **MCE** (7.0/10): Critical for safety, identifies worst-case gaps
6. **Calibration Slope** (6.5/10): Robust parametric summary, trend analysis
7. **Spiegelhalter Z** (6.0/10): Formal statistical testing, sample size sensitive
8. **Standard ECE** (5.5/10): Intuitive but bin-sensitive and biased

**Interpretability Ranking** (Practitioner understanding and actionability):
1. **ECE variants** (9.0/10): Direct interpretation as expected calibration gap
2. **Calibration Slope/Intercept** (8.5/10): Clear geometric meaning, actionable
3. **Brier Decomposition** (8.0/10): Separates reliability, resolution, uncertainty
4. **OCE/UCE** (7.5/10): Directional calibration information
5. **MCE** (7.0/10): Worst-case interpretation, important for safety
6. **Selective Risk@Coverage** (6.5/10): Operationally relevant but requires context
7. **NLL** (5.0/10): Theoretically principled but less intuitive
8. **Spiegelhalter Z** (4.5/10): Statistical but may not indicate practical significance

**Robustness Ranking** (Performance under various conditions):
1. **AURC** (9.0/10): Integrates across coverage levels, robust to thresholds
2. **Classwise ECE** (8.5/10): Handles imbalance better than top-label metrics
3. **Calibration Slope** (8.0/10): Parametric approach robust to outliers
4. **Selective Risk@Coverage** (7.5/10): Direct operational relevance
5. **Debiased ECE** (7.0/10): Addresses statistical biases
6. **NLL** (6.5/10): Can be dominated by extreme cases
7. **MCE** (5.5/10): High variance, sensitive to single bad bins
8. **Standard ECE** (5.0/10): Sensitive to binning choices

### 7.2 Visualization Criteria Rankings

**Interpretability Ranking**:
1. **Reliability Diagrams** (9.5/10): Most intuitive calibration visualization
2. **Risk-Coverage Curves** (8.5/10): Direct selective prediction insights
3. **Temperature Sweeps** (8.0/10): Clear parameter guidance
4. **Boxplots by Agreement** (7.5/10): Clear subgroup comparisons
5. **ROC/PR Overlays** (7.0/10): Standard discriminative performance
6. **Confidence Histograms** (6.5/10): Distribution patterns per class
7. **Correlation Heatmaps** (6.0/10): Comprehensive but complex relationships
8. **Cumulative Gain Charts** (5.5/10): Business value but domain-specific

**Diagnostic Value Ranking**:
1. **Correlation Heatmaps** (9.0/10): Reveals hidden relationships
2. **Boxplots by Agreement** (8.5/10): Validates auxiliary signals
3. **Temperature Sweeps** (8.0/10): Systematic bias diagnosis
4. **Confidence-Error Curves** (7.5/10): Direct confidence quality assessment
5. **Reliability Diagrams** (7.0/10): Standard calibration diagnosis
6. **Risk-Coverage Curves** (6.5/10): Selective prediction capability
7. **Confidence Histograms** (6.0/10): Per-class pattern analysis
8. **ROC/PR Overlays** (5.5/10): Discriminative performance only

### 7.3 Implementation Complexity

**Low Complexity** (Direct implementation, fast computation):
- NLL, Brier Score, Standard ECE
- Basic reliability diagrams, confidence histograms
- Temperature sweeps, ROC/PR curves

**Medium Complexity** (Careful implementation required):
- Classwise ECE, AURC, Selective Risk
- Correlation heatmaps, boxplots by subgroups
- Risk-coverage curves, confidence-error curves

**High Complexity** (Advanced algorithms, specialized libraries):
- Debiased ECE, TACE/KECE, Conformal Prediction
- Adaptive reliability diagrams, cumulative gain charts
- Advanced ensemble uncertainty methods

### 7.4 Combined Recommendations

**Tier 1: Essential Metrics** (Every deployment):
- **NLL**: Primary proper scoring rule for overall assessment
- **Classwise ECE**: Handles imbalance, reveals per-class issues  
- **Calibration Slope**: Parametric summary for trend monitoring
- **AURC**: Selective prediction capability assessment

**Tier 1: Essential Visualizations**:
- **Reliability Diagrams**: Primary calibration assessment
- **Boxplots by Agreement**: Subgroup validation and analysis
- **Temperature Sweeps**: Systematic bias diagnosis
- **Risk-Coverage Curves**: Selective prediction evaluation

**Tier 2: Diagnostic Metrics** (Deep analysis):
- **MCE**: Safety-critical worst-case assessment
- **OCE/UCE**: Directional calibration insights
- **Brier Score**: Research and comparative analysis
- **Selective Risk@Coverage**: Operational threshold setting

**Tier 2: Diagnostic Visualizations**:
- **Correlation Heatmaps**: Relationship discovery
- **Confidence Histograms**: Per-class pattern analysis
- **ROC/PR Overlays**: Discriminative performance comparison
- **Confidence-Error Curves**: Direct confidence validation

**Tier 3: Specialized Applications** (Domain-specific):
- **Spiegelhalter Z**: Formal statistical requirements
- **Cost-sensitive Risk**: Business alignment
- **Conformal Coverage**: Regulatory compliance
- **TACE/KECE**: Advanced research applications

## 8. Practitioner Checklist ✅

### Phase 1: Foundation Setup
- [ ] **Data Preparation**
  - [ ] Implement stratified train/val/test splits (60/20/20)  
  - [ ] Generate agreement labels or auxiliary context signals
  - [ ] Design multi-token verbalizers with length normalization
  - [ ] Validate class distribution and imbalance patterns
- [ ] **Baseline Establishment**
  - [ ] Compute raw softmax probabilities as baseline
  - [ ] Measure all Tier 1 metrics on validation set
  - [ ] Establish acceptable performance thresholds
  - [ ] Document baseline calibration patterns

### Phase 2: Calibration Implementation
- [ ] **Method Selection**
  - [ ] Start with temperature scaling (universal first step)
  - [ ] Implement contextual calibration if subgroups identified
  - [ ] Add prompt ensembling for robustness (if budget allows)
  - [ ] Consider evidential/conformal for specialized needs
- [ ] **Optimization Process**
  - [ ] Fit calibration parameters on validation set only
  - [ ] Use NLL as primary optimization target
  - [ ] Cross-validate calibration method selection
  - [ ] Validate improvements on held-out test set

### Phase 3: Evaluation Pipeline
- [ ] **Core Metrics Implementation**
  - [ ] NLL: Primary proper scoring rule
  - [ ] Classwise ECE: Per-class calibration assessment
  - [ ] Calibration slope/intercept: Parametric summary
  - [ ] MCE: Worst-case calibration gaps
  - [ ] AURC: Selective prediction capability
- [ ] **Subgroup Analysis**
  - [ ] Slice all metrics by agreement status
  - [ ] Analyze per-class calibration patterns
  - [ ] Identify systematic miscalibration sources
  - [ ] Document class-specific recommendations

### Phase 4: Visualization Suite
- [ ] **Primary Visualizations**
  - [ ] Reliability diagrams (overall, per-class, by agreement)
  - [ ] Risk-coverage curves with AURC shading
  - [ ] Temperature sweep analysis
  - [ ] Boxplots by agreement status
- [ ] **Diagnostic Visualizations**
  - [ ] Confidence/entropy/margin distributions by agreement
  - [ ] Score correlation heatmaps
  - [ ] Per-class confidence histograms
  - [ ] ROC/PR curves for discrimination analysis

### Phase 5: Production Deployment
- [ ] **Threshold Configuration**
  - [ ] Set selective prediction thresholds based on risk tolerance
  - [ ] Configure class-specific handling for imbalanced classes
  - [ ] Establish confidence-based routing rules
  - [ ] Document threshold rationale and business alignment
- [ ] **Monitoring Infrastructure**
  - [ ] Daily: NLL, Classwise ECE, MCE tracking
  - [ ] Weekly: Calibration slope/intercept trend analysis
  - [ ] Monthly: Full metric suite and visualization refresh
  - [ ] Quarterly: Calibration method reevaluation

### Phase 6: Maintenance & Iteration
- [ ] **Drift Detection**
  - [ ] Monitor temperature sweep patterns for calibration drift
  - [ ] Track agreement-sliced metrics for subgroup fairness
  - [ ] Alert on significant metric degradation
  - [ ] Investigate and remediate drift sources
- [ ] **Continuous Improvement**
  - [ ] A/B testing of calibration methods
  - [ ] Incorporation of new auxiliary signals
  - [ ] Calibration method updates based on new research
  - [ ] Business metric alignment validation

## 9. References

**Core Calibration Theory:**
- Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017). On calibration of modern neural networks. *ICML*.
- Platt, J. (1999). Probabilistic outputs for support vector machines. *Advances in Large Margin Classifiers*.
- Niculescu-Mizil, A., & Caruana, R. (2005). Predicting good probabilities with supervised learning. *ICML*.

**Advanced Calibration Methods:**
- Kull, M., Silva Filho, T., & Flach, P. (2017). Beta calibration: A well-founded and easily implemented improvement on logistic calibration. *AISTATS*.
- Kull, M., et al. (2019). Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration. *NeurIPS*.
- Kumar, A., Liang, P. S., & Ma, T. (2019). Verified uncertainty calibration. *NeurIPS*.

**Uncertainty Quantification:**
- Lakshminarayanan, B., Pritzel, A., & Blundell, C. (2017). Simple and scalable predictive uncertainty estimation using deep ensembles. *NeurIPS*.
- Malinin, A., & Gales, M. (2018). Predictive uncertainty estimation via prior networks. *NeurIPS*.
- Sensoy, M., Kaplan, L., & Kandemir, M. (2018). Evidential deep learning to quantify classification uncertainty. *NeurIPS*.

**Conformal Prediction:**
- Vovk, V., Gammerman, A., & Shafer, G. (2005). *Algorithmic Learning in a Random World*. Springer.
- Angelopoulos, A. N., & Bates, S. (2021). A gentle introduction to conformal prediction and distribution-free uncertainty quantification. *arXiv preprint*.

**Evaluation Metrics:**
- Murphy, A. H. (1973). A new vector partition of the probability score. *Journal of Applied Meteorology*.
- Brier, G. W. (1950). Verification of forecasts expressed in terms of probability. *Monthly Weather Review*.
- Nixon, J., et al. (2019). Measuring calibration in deep learning. *CVPR Workshops*.

---

*Report generated automatically by email5_llm_confidence.py - All figures saved under output/figures/*
```


## File 2: email5_llm_confidence.py

```python
#!/usr/bin/env python3
"""
LLM-Based Multi-Class Email Classification Confidence Analysis

This script implements comprehensive confidence scoring, calibration, and evaluation
methods for LLM-based email classification on the Email5 dataset.

Usage:
    python email5_llm_confidence.py --mode simulated --samples 500 --random_seed 42

Author: Generated for Email5 Confidence Analysis
Date: 2025-08-28
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import argparse
import os
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
from scipy import stats
from scipy.optimize import minimize_scalar
from scipy.interpolate import interp1d
from sklearn.model_selection import train_test_split
from sklearn.calibration import CalibratedClassifierCV
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support, 
    roc_auc_score, average_precision_score, brier_score_loss,
    confusion_matrix, roc_curve, precision_recall_curve
)
import json
import logging

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Set style for all plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")


@dataclass
class EmailSample:
    """Email sample with metadata"""
    text: str
    true_class: int
    class_name: str
    agreement: int  # 1=agree, 0=disagree
    logprobs: Dict[int, float]  # class -> logprob
    
    
@dataclass 
class ConfidenceResults:
    """Results from confidence estimation"""
    raw_probs: np.ndarray
    calibrated_probs: np.ndarray
    confidence_scores: np.ndarray
    uncertainty_scores: np.ndarray
    method_name: str
    

def seed_all(seed: int = 42):
    """Set all random seeds for reproducibility"""
    np.random.seed(seed)
    import random
    random.seed(seed)
    # If using torch/tensorflow, set their seeds too
    

def make_email5_dataset(n_samples: int = 500, 
                       random_seed: int = 42) -> Tuple[List[EmailSample], Dict]:
    """
    Generate simulated Email5 dataset with realistic class distribution and agreement patterns
    
    Args:
        n_samples: Total number of samples
        random_seed: Random seed for reproducibility
        
    Returns:
        Tuple of (samples, metadata)
    """
    seed_all(random_seed)
    
    # Class configuration
    classes = {
        0: {"name": "Spam", "freq": 0.35, "agreement_rate": 0.80},
        1: {"name": "Promotions", "freq": 0.25, "agreement_rate": 0.70}, 
        2: {"name": "Social", "freq": 0.18, "agreement_rate": 0.60},
        3: {"name": "Updates", "freq": 0.17, "agreement_rate": 0.60},
        4: {"name": "Forums", "freq": 0.05, "agreement_rate": 0.50}
    }
    
    samples = []
    
    for i in range(n_samples):
        # Sample class according to frequency
        class_probs = [classes[k]["freq"] for k in range(5)]
        true_class = np.random.choice(5, p=class_probs)
        class_name = classes[true_class]["name"]
        
        # Sample agreement based on class-specific rate
        agreement = np.random.binomial(1, classes[true_class]["agreement_rate"])
        
        # Generate text (placeholder)
        text = f"Email {i+1}: {class_name} content..."
        
        # Simulate logprobs with class-dependent quality and agreement effects
        logprobs = simulate_logprobs_for_sample(true_class, agreement, classes)
        
        sample = EmailSample(
            text=text,
            true_class=true_class,
            class_name=class_name,
            agreement=agreement,
            logprobs=logprobs
        )
        samples.append(sample)
    
    metadata = {
        "n_samples": n_samples,
        "classes": classes,
        "class_names": [classes[k]["name"] for k in range(5)],
        "overall_agreement_rate": np.mean([s.agreement for s in samples])
    }
    
    logger.info(f"Generated {n_samples} samples with {metadata['overall_agreement_rate']:.3f} agreement rate")
    
    return samples, metadata


def simulate_logprobs_for_sample(true_class: int, agreement: int, 
                               classes: Dict) -> Dict[int, float]:
    """
    Simulate realistic logprobs for a sample based on true class and agreement
    
    Args:
        true_class: True class index
        agreement: Agreement indicator (1=agree, 0=disagree)  
        classes: Class configuration
        
    Returns:
        Dictionary mapping class -> logprob
    """
    n_classes = len(classes)
    
    # Base quality depends on class frequency (more frequent = better learned)
    freq = classes[true_class]["freq"] 
    base_quality = 0.3 + 0.5 * freq  # Range: 0.3 (Forums) to 0.475 (Spam)
    
    # Agreement effect: disagreed samples are harder
    quality = base_quality * (1.2 if agreement else 0.6)
    
    # Generate logits with class-dependent noise
    logits = np.random.normal(0, 1, n_classes)
    
    # Boost true class logit based on quality
    logits[true_class] += np.random.normal(2.0 * quality, 0.5)
    
    # Add some systematic overconfidence (will be corrected by calibration)
    overconfidence_factor = 1.5
    logits *= overconfidence_factor
    
    # Convert to logprobs (log of softmax)
    log_probs = logits - np.log(np.sum(np.exp(logits)))
    
    return {k: log_probs[k] for k in range(n_classes)}


def split_data(samples: List[EmailSample], 
               train_size: float = 0.6,
               val_size: float = 0.2,
               test_size: float = 0.2,
               random_seed: int = 42) -> Tuple[List[EmailSample], List[EmailSample], List[EmailSample]]:
    """Split data into train/val/test with stratification"""
    
    # Extract labels for stratification
    labels = [s.true_class for s in samples]
    indices = list(range(len(samples)))
    
    # First split: train vs (val+test)
    train_idx, temp_idx = train_test_split(
        indices, test_size=(val_size + test_size), 
        stratify=labels, random_state=random_seed
    )
    
    # Second split: val vs test
    temp_labels = [labels[i] for i in temp_idx]
    val_idx, test_idx = train_test_split(
        temp_idx, test_size=test_size/(val_size + test_size),
        stratify=temp_labels, random_state=random_seed
    )
    
    train_samples = [samples[i] for i in train_idx]
    val_samples = [samples[i] for i in val_idx] 
    test_samples = [samples[i] for i in test_idx]
    
    logger.info(f"Split: Train={len(train_samples)}, Val={len(val_samples)}, Test={len(test_samples)}")
    
    return train_samples, val_samples, test_samples


def get_verbalizers() -> Dict[int, List[str]]:
    """Get multi-token verbalizers for each class"""
    return {
        0: ["spam", "junk"],  # Spam
        1: ["promotion", "marketing", "advertisement"], # Promotions  
        2: ["social", "personal"], # Social
        3: ["update", "notification"], # Updates
        4: ["forum", "discussion"] # Forums
    }


def aggregate_to_class_probs(samples: List[EmailSample], 
                           verbalizers: Dict[int, List[str]],
                           aggregation: str = "length_norm") -> np.ndarray:
    """
    Aggregate token-level logprobs to class-level probabilities
    
    Args:
        samples: List of email samples with logprobs
        verbalizers: Mapping from class to verbalizer tokens
        aggregation: "sum", "avg", or "length_norm"
        
    Returns:
        Array of shape (n_samples, n_classes) with probabilities
    """
    n_samples = len(samples)
    n_classes = len(verbalizers)
    
    class_logprobs = np.zeros((n_samples, n_classes))
    
    for i, sample in enumerate(samples):
        for cls in range(n_classes):
            # For simulation, we already have class-level logprobs
            # In real LLM, would aggregate over verbalizer tokens here
            logprob = sample.logprobs[cls]
            
            if aggregation == "length_norm":
                # Normalize by verbalizer length
                length = len(verbalizers[cls])
                class_logprobs[i, cls] = logprob / length
            elif aggregation == "avg":
                class_logprobs[i, cls] = logprob
            else:  # sum
                class_logprobs[i, cls] = logprob
    
    # Convert logprobs to probabilities
    probs = np.exp(class_logprobs)
    probs = probs / probs.sum(axis=1, keepdims=True)  # Normalize
    
    return probs


# =============================================================================
# CONFIDENCE SCORING METHODS 
# =============================================================================

def raw_logprobs(probs: np.ndarray) -> ConfidenceResults:
    """Raw softmax probabilities as confidence scores"""
    confidence = np.max(probs, axis=1)
    uncertainty = 1 - confidence
    
    return ConfidenceResults(
        raw_probs=probs,
        calibrated_probs=probs.copy(),
        confidence_scores=confidence,
        uncertainty_scores=uncertainty,
        method_name="Raw Logprobs"
    )


def normalized_logprobs(probs: np.ndarray) -> ConfidenceResults:
    """Z-score normalized probabilities"""
    # Normalize each sample's probabilities
    logits = np.log(probs + 1e-8)
    
    normalized_logits = np.zeros_like(logits)
    for i in range(len(logits)):
        mean_logit = np.mean(logits[i])
        std_logit = np.std(logits[i]) + 1e-8
        normalized_logits[i] = (logits[i] - mean_logit) / std_logit
    
    # Convert back to probabilities
    normalized_probs = np.exp(normalized_logits)
    normalized_probs = normalized_probs / normalized_probs.sum(axis=1, keepdims=True)
    
    confidence = np.max(normalized_probs, axis=1)
    uncertainty = 1 - confidence
    
    return ConfidenceResults(
        raw_probs=probs,
        calibrated_probs=normalized_probs,
        confidence_scores=confidence,
        uncertainty_scores=uncertainty,
        method_name="Normalized Logprobs"
    )


def logprob_margin(probs: np.ndarray) -> ConfidenceResults:
    """Margin between top two predictions as confidence"""
    sorted_probs = np.sort(probs, axis=1)
    margin = sorted_probs[:, -1] - sorted_probs[:, -2]
    uncertainty = 1 - margin
    
    return ConfidenceResults(
        raw_probs=probs,
        calibrated_probs=probs.copy(),
        confidence_scores=margin,
        uncertainty_scores=uncertainty, 
        method_name="Logprob Margin"
    )


def topk_margin(probs: np.ndarray, k: int = 3) -> ConfidenceResults:
    """Top-k margin confidence"""
    sorted_probs = np.sort(probs, axis=1)
    top1 = sorted_probs[:, -1]
    
    if k <= probs.shape[^1]:
        topk_avg = np.mean(sorted_probs[:, -k:-1], axis=1) if k > 1 else sorted_probs[:, -2]
    else:
        topk_avg = np.mean(sorted_probs[:, :-1], axis=1)
        
    margin = top1 - topk_avg
    uncertainty = 1 - margin
    
    return ConfidenceResults(
        raw_probs=probs,
        calibrated_probs=probs.copy(),
        confidence_scores=margin,
        uncertainty_scores=uncertainty,
        method_name=f"Top-{k} Margin"
    )


def entropy_score(probs: np.ndarray) -> ConfidenceResults:
    """Shannon entropy as uncertainty measure"""
    entropy = -np.sum(probs * np.log(probs + 1e-8), axis=1)
    max_entropy = np.log(probs.shape[^1])
    normalized_entropy = entropy / max_entropy
    confidence = 1 - normalized_entropy
    
    return ConfidenceResults(
        raw_probs=probs,
        calibrated_probs=probs.copy(),
        confidence_scores=confidence,
        uncertainty_scores=entropy,
        method_name="Entropy Score"
    )


def energy_score(probs: np.ndarray, temperature: float = 1.0) -> ConfidenceResults:
    """Energy-based confidence scores"""
    logits = np.log(probs + 1e-8)
    energy = -temperature * np.log(np.sum(np.exp(logits / temperature), axis=1))
    
    # Normalize energy to [0, 1] range (lower energy = higher confidence)
    min_energy, max_energy = np.min(energy), np.max(energy)
    if max_energy > min_energy:
        confidence = 1 - (energy - min_energy) / (max_energy - min_energy)
    else:
        confidence = np.ones_like(energy)
        
    uncertainty = 1 - confidence
    
    return ConfidenceResults(
        raw_probs=probs,
        calibrated_probs=probs.copy(), 
        confidence_scores=confidence,
        uncertainty_scores=uncertainty,
        method_name="Energy Score"
    )


def token_aggregation(samples: List[EmailSample], 
                     verbalizers: Dict[int, List[str]],
                     method: str = "length_norm") -> ConfidenceResults:
    """Token-level aggregation methods (placeholder - calls aggregate_to_class_probs)"""
    probs = aggregate_to_class_probs(samples, verbalizers, method)
    confidence = np.max(probs, axis=1)
    uncertainty = 1 - confidence
    
    return ConfidenceResults(
        raw_probs=probs,
        calibrated_probs=probs,
        confidence_scores=confidence,
        uncertainty_scores=uncertainty,
        method_name=f"Token Aggregation ({method})"
    )


def prompt_ensemble(probs_list: List[np.ndarray]) -> ConfidenceResults:
    """Ensemble predictions from multiple prompts"""
    if len(probs_list) == 1:
        return raw_logprobs(probs_list[^0])
        
    # Average probabilities across ensemble members
    ensemble_probs = np.mean(probs_list, axis=0)
    
    # Confidence as max probability
    confidence = np.max(ensemble_probs, axis=1)
    
    # Uncertainty as ensemble variance (mutual information proxy)
    ensemble_variance = np.var(probs_list, axis=0)
    uncertainty = np.mean(ensemble_variance, axis=1)
    
    return ConfidenceResults(
        raw_probs=probs_list[^0],
        calibrated_probs=ensemble_probs,
        confidence_scores=confidence, 
        uncertainty_scores=uncertainty,
        method_name="Prompt Ensemble"
    )


def llm_as_judge_stub(probs: np.ndarray, samples: List[EmailSample]) -> ConfidenceResults:
    """Placeholder for LLM-as-judge confidence scoring"""
    # Simulate judge scores that correlate with max probability but add noise
    max_probs = np.max(probs, axis=1)
    judge_noise = np.random.normal(0, 0.1, len(max_probs))
    judge_confidence = np.clip(max_probs + judge_noise, 0, 1)
    
    uncertainty = 1 - judge_confidence
    
    return ConfidenceResults(
        raw_probs=probs,
        calibrated_probs=probs.copy(),
        confidence_scores=judge_confidence,
        uncertainty_scores=uncertainty,
        method_name="LLM-as-Judge (Stub)"
    )


def memory_score(probs: np.ndarray, samples: List[EmailSample], 
                memory_samples: List[EmailSample]) -> ConfidenceResults:
    """Memory/retrieval-based confidence (simplified)"""
    # Simulate memory-based confidence using class frequencies
    class_counts = {}
    for sample in memory_samples:
        class_counts[sample.true_class] = class_counts.get(sample.true_class, 0) + 1
    
    total_memory = len(memory_samples)
    confidence_scores = []
    
    for i, sample in enumerate(samples):
        predicted_class = np.argmax(probs[i])
        # Confidence based on how much training data we had for this class
        class_freq = class_counts.get(predicted_class, 1) / total_memory
        memory_confidence = min(0.9, 0.3 + class_freq * 2)  # Scale to reasonable range
        confidence_scores.append(memory_confidence)
    
    confidence_scores = np.array(confidence_scores)
    uncertainty = 1 - confidence_scores
    
    return ConfidenceResults(
        raw_probs=probs,
        calibrated_probs=probs.copy(),
        confidence_scores=confidence_scores,
        uncertainty_scores=uncertainty,
        method_name="Memory Score"
    )


# =============================================================================
# CALIBRATION METHODS
# =============================================================================

def temperature_scale(probs: np.ndarray, labels: np.ndarray, 
                     val_probs: np.ndarray, val_labels: np.ndarray) -> Tuple[np.ndarray, float]:
    """
    Temperature scaling calibration
    
    Args:
        probs: Training probabilities
        labels: Training labels  
        val_probs: Validation probabilities
        val_labels: Validation labels
        
    Returns:
        Tuple of (calibrated_probs, optimal_temperature)
    """
    # Convert probabilities back to logits
    logits = np.log(probs + 1e-8)
    val_logits = np.log(val_probs + 1e-8)
    
    def nll_loss(temperature):
        """Negative log-likelihood loss for given temperature"""
        scaled_logits = val_logits / temperature
        # Stable softmax computation
        max_logits = np.max(scaled_logits, axis=1, keepdims=True)
        exp_logits = np.exp(scaled_logits - max_logits)
        log_sum_exp = np.log(np.sum(exp_logits, axis=1)) + max_logits.squeeze()
        
        nll = 0
        for i in range(len(val_labels)):
            nll -= scaled_logits[i, val_labels[i]] - log_sum_exp[i]
        return nll / len(val_labels)
    
    # Optimize temperature on validation set
    result = minimize_scalar(nll_loss, bounds=(0.1, 10.0), method='bounded')
    optimal_temp = result.x
    
    # Apply optimal temperature to get calibrated probabilities
    scaled_logits = logits / optimal_temp
    max_logits = np.max(scaled_logits, axis=1, keepdims=True)
    exp_logits = np.exp(scaled_logits - max_logits)
    calibrated_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)
    
    return calibrated_probs, optimal_temp


def platt_scaling(probs: np.ndarray, labels: np.ndarray,
                 val_probs: np.ndarray, val_labels: np.ndarray) -> np.ndarray:
    """Platt scaling calibration (sigmoid)"""
    from sklearn.linear_model import LogisticRegression
    
    # Use max probability as feature for binary calibration
    max_probs = np.max(val_probs, axis=1).reshape(-1, 1)
    predicted_labels = np.argmax(val_probs, axis=1)
    
    # Binary problem: correct vs incorrect
    binary_labels = (predicted_labels == val_labels).astype(int)
    
    # Fit sigmoid
    platt_model = LogisticRegression()
    platt_model.fit(max_probs, binary_labels)
    
    # Apply to training data
    train_max_probs = np.max(probs, axis=1).reshape(-1, 1)
    calibrated_confidence = platt_model.predict_proba(train_max_probs)[:, 1]
    
    # Scale all probabilities by the confidence adjustment
    calibrated_probs = probs.copy()
    for i in range(len(probs)):
        max_prob = np.max(probs[i])
        if max_prob > 0:
            scale_factor = calibrated_confidence[i] / max_prob
            calibrated_probs[i] *= scale_factor
            calibrated_probs[i] /= np.sum(calibrated_probs[i])  # Renormalize
    
    return calibrated_probs


def isotonic_calibration(probs: np.ndarray, labels: np.ndarray,
                        val_probs: np.ndarray, val_labels: np.ndarray) -> np.ndarray:
    """Isotonic regression calibration"""
    # Use max probability and correctness for calibration
    max_probs = np.max(val_probs, axis=1)
    predicted_labels = np.argmax(val_probs, axis=1)
    correctness = (predicted_labels == val_labels).astype(float)
    
    # Fit isotonic regression
    iso_reg = IsotonicRegression(out_of_bounds='clip')
    iso_reg.fit(max_probs, correctness)
    
    # Apply to training data
    train_max_probs = np.max(probs, axis=1)
    calibrated_confidence = iso_reg.predict(train_max_probs)
    
    # Scale probabilities
    calibrated_probs = probs.copy()
    for i in range(len(probs)):
        max_prob = np.max(probs[i])
        if max_prob > 0:
            scale_factor = calibrated_confidence[i] / max_prob
            calibrated_probs[i] *= scale_factor
            calibrated_probs[i] /= np.sum(calibrated_probs[i])
    
    return calibrated_probs


def histogram_binning(probs: np.ndarray, labels: np.ndarray,
                     val_probs: np.ndarray, val_labels: np.ndarray,
                     n_bins: int = 15) -> np.ndarray:
    """Histogram binning calibration"""
    # Get confidence and accuracy for validation set
    val_confidence = np.max(val_probs, axis=1)
    val_predicted = np.argmax(val_probs, axis=1)
    val_correct = (val_predicted == val_labels).astype(float)
    
    # Create bins
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    bin_lowers = bin_boundaries[:-1]
    bin_uppers = bin_boundaries[1:]
    
    # Compute bin accuracies
    bin_accuracies = []
    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
        in_bin = (val_confidence > bin_lower) & (val_confidence <= bin_upper)
        if np.sum(in_bin) > 0:
            bin_accuracy = np.mean(val_correct[in_bin])
        else:
            bin_accuracy = 0.5  # Default for empty bins
        bin_accuracies.append(bin_accuracy)
    
    # Apply to training data
    train_confidence = np.max(probs, axis=1)
    calibrated_confidence = np.zeros_like(train_confidence)
    
    for i, conf in enumerate(train_confidence):
        bin_idx = np.digitize(conf, bin_boundaries) - 1
        bin_idx = max(0, min(bin_idx, len(bin_accuracies) - 1))
        calibrated_confidence[i] = bin_accuracies[bin_idx]
    
    # Scale probabilities
    calibrated_probs = probs.copy()
    for i in range(len(probs)):
        max_prob = np.max(probs[i])
        if max_prob > 0:
            scale_factor = calibrated_confidence[i] / max_prob
            calibrated_probs[i] *= scale_factor
            calibrated_probs[i] /= np.sum(calibrated_probs[i])
    
    return calibrated_probs


def spline_calibration(probs: np.ndarray, labels: np.ndarray,
                      val_probs: np.ndarray, val_labels: np.ndarray) -> np.ndarray:
    """Spline-based calibration"""
    from scipy.interpolate import UnivariateSpline
    
    val_confidence = np.max(val_probs, axis=1)
    val_predicted = np.argmax(val_probs, axis=1)
    val_correct = (val_predicted == val_labels).astype(float)
    
    # Sort by confidence for spline fitting
    sort_idx = np.argsort(val_confidence)
    sorted_conf = val_confidence[sort_idx]
    sorted_acc = val_correct[sort_idx]
    
    # Fit spline (with smoothing)
    try:
        spline = UnivariateSpline(sorted_conf, sorted_acc, s=len(sorted_conf)*0.01)
    except:
        # Fallback to linear interpolation if spline fails
        spline = interp1d(sorted_conf, sorted_acc, kind='linear', 
                         bounds_error=False, fill_value=(sorted_acc[^0], sorted_acc[-1]))
    
    # Apply to training data
    train_confidence = np.max(probs, axis=1)
    calibrated_confidence = np.clip(spline(train_confidence), 0, 1)
    
    # Scale probabilities
    calibrated_probs = probs.copy()
    for i in range(len(probs)):
        max_prob = np.max(probs[i])
        if max_prob > 0:
            scale_factor = calibrated_confidence[i] / max_prob
            calibrated_probs[i] *= scale_factor
            calibrated_probs[i] /= np.sum(calibrated_probs[i])
    
    return calibrated_probs


def beta_calibration(probs: np.ndarray, labels: np.ndarray,
                    val_probs: np.ndarray, val_labels: np.ndarray) -> np.ndarray:
    """Beta calibration (simplified implementation)"""
    from scipy.optimize import minimize
    from scipy.special import logit, expit
    
    val_confidence = np.max(val_probs, axis=1)
    val_predicted = np.argmax(val_probs, axis=1)
    val_correct = (val_predicted == val_labels).astype(float)
    
    # Fit beta calibration parameters
    def beta_loss(params):
        a, b, c = params
        logit_conf = logit(np.clip(val_confidence, 1e-8, 1-1e-8))
        calibrated_logits = a * logit_conf + b
        calibrated_probs = expit(calibrated_logits)
        return -np.sum(val_correct * np.log(calibrated_probs + 1e-8) + 
                      (1 - val_correct) * np.log(1 - calibrated_probs + 1e-8))
    
    # Optimize parameters
    result = minimize(beta_loss, [1.0, 0.0, 0.0], method='BFGS')
    a, b, c = result.x
    
    # Apply to training data
    train_confidence = np.max(probs, axis=1)
    logit_conf = logit(np.clip(train_confidence, 1e-8, 1-1e-8))
    calibrated_logits = a * logit_conf + b
    calibrated_confidence = expit(calibrated_logits)
    
    # Scale probabilities
    calibrated_probs = probs.copy()
    for i in range(len(probs)):
        max_prob = np.max(probs[i])
        if max_prob > 0:
            scale_factor = calibrated_confidence[i] / max_prob
            calibrated_probs[i] *= scale_factor
            calibrated_probs[i] /= np.sum(calibrated_probs[i])
    
    return calibrated_probs


def vector_scaling(probs: np.ndarray, labels: np.ndarray,
                  val_probs: np.ndarray, val_labels: np.ndarray) -> np.ndarray:
    """Vector scaling calibration"""
    # Convert to logits
    logits = np.log(probs + 1e-8)
    val_logits = np.log(val_probs + 1e-8)
    
    # Optimize class-specific temperatures
    n_classes = probs.shape[^1]
    
    def nll_loss(temperatures):
        scaled_logits = val_logits / temperatures.reshape(1, -1)
        max_logits = np.max(scaled_logits, axis=1, keepdims=True)
        exp_logits = np.exp(scaled_logits - max_logits)
        log_sum_exp = np.log(np.sum(exp_logits, axis=1)) + max_logits.squeeze()
        
        nll = 0
        for i in range(len(val_labels)):
            nll -= scaled_logits[i, val_labels[i]] - log_sum_exp[i]
        return nll / len(val_labels)
    
    # Optimize temperatures
    from scipy.optimize import minimize
    result = minimize(nll_loss, np.ones(n_classes), 
                     bounds=[(0.1, 10.0)] * n_classes, method='L-BFGS-B')
    optimal_temps = result.x
    
    # Apply scaling
    scaled_logits = logits / optimal_temps.reshape(1, -1)
    max_logits = np.max(scaled_logits, axis=1, keepdims=True)
    exp_logits = np.exp(scaled_logits - max_logits)
    calibrated_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)
    
    return calibrated_probs


def matrix_scaling(probs: np.ndarray, labels: np.ndarray,
                  val_probs: np.ndarray, val_labels: np.ndarray) -> np.ndarray:
    """Matrix scaling calibration (simplified)"""
    # For simplicity, implement as diagonal matrix scaling (equivalent to vector scaling)
    return vector_scaling(probs, labels, val_probs, val_labels)


def dirichlet_scaling(probs: np.ndarray, labels: np.ndarray,
                     val_probs: np.ndarray, val_labels: np.ndarray) -> np.ndarray:
    """Dirichlet scaling calibration (simplified)"""
    # Simplified version - use temperature scaling on the logit space
    return temperature_scale(probs, labels, val_probs, val_labels)[^0]


def contextual_calibration(probs: np.ndarray, labels: np.ndarray,
                         val_probs: np.ndarray, val_labels: np.ndarray,
                         agreement: np.ndarray, val_agreement: np.ndarray) -> Tuple[np.ndarray, Dict]:
    """
    Contextual calibration based on agreement signal
    
    Returns:
        Tuple of (calibrated_probs, context_info)
    """
    # Split validation data by agreement
    agree_mask = val_agreement == 1
    disagree_mask = val_agreement == 0
    
    contexts = {}
    calibrated_probs = probs.copy()
    
    for context_name, val_mask, train_mask in [
        ("agreement", agree_mask, agreement == 1),
        ("disagreement", disagree_mask, agreement == 0)
    ]:
        if np.sum(val_mask) > 10:  # Need sufficient validation samples
            # Fit temperature scaling for this context
            context_val_probs = val_probs[val_mask]
            context_val_labels = val_labels[val_mask]
            context_train_probs = probs[train_mask]
            context_train_labels = labels[train_mask]
            
            if len(context_val_probs) > 0 and len(context_train_probs) > 0:
                context_calibrated, temp = temperature_scale(
                    context_train_probs, context_train_labels,
                    context_val_probs, context_val_labels
                )
                calibrated_probs[train_mask] = context_calibrated
                contexts[context_name] = {"temperature": temp, "samples": np.sum(train_mask)}
    
    return calibrated_probs, contexts


def evidential_dirichlet(probs: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, Dict]:
    """
    Evidential Dirichlet calibration (simplified)
    
    Returns:
        Tuple of (calibrated_probs, uncertainty_info)
    """
    # Simulate evidential parameters
    # In practice, this would require training a specific evidential network
    n_classes = probs.shape[^1]
    
    # Convert probabilities to concentration parameters (evidence + 1)
    evidence = probs * 10  # Scale factor for evidence
    alpha = evidence + 1
    
    # Dirichlet mean (predictive probability)
    S = np.sum(alpha, axis=1, keepdims=True)
    calibrated_probs = alpha / S
    
    # Uncertainty measures
    total_uncertainty = n_classes / S.squeeze()
    aleatoric = np.sum(calibrated_probs * (1 - calibrated_probs), axis=1) / (S.squeeze() + 1)
    epistemic = total_uncertainty - aleatoric
    
    uncertainty_info = {
        "total_uncertainty": total_uncertainty,
        "aleatoric": aleatoric,
        "epistemic": epistemic,
        "concentration": alpha
    }
    
    return calibrated_probs, uncertainty_info


def conformal_prediction(probs: np.ndarray, labels: np.ndarray,
                        val_probs: np.ndarray, val_labels: np.ndarray,
                        alpha: float = 0.1) -> Tuple[List[List[int]], Dict]:
    """
    Conformal prediction for set-valued predictions
    
    Args:
        alpha: Miscoverage rate (1-alpha = coverage level)
        
    Returns:
        Tuple of (prediction_sets, conformal_info)
    """
    # Nonconformity scores: 1 - probability of true class
    val_predicted = np.argmax(val_probs, axis=1)
    nonconformity_scores = []
    
    for i in range(len(val_labels)):
        score = 1 - val_probs[i, val_labels[i]]
        nonconformity_scores.append(score)
    
    nonconformity_scores = np.array(nonconformity_scores)
    
    # Quantile for conformal threshold
    n_val = len(nonconformity_scores)
    quantile_level = np.ceil((n_val + 1) * (1 - alpha)) / n_val
    threshold = np.quantile(nonconformity_scores, quantile_level)
    
    # Generate prediction sets for training data
    prediction_sets = []
    set_sizes = []
    
    for i in range(len(probs)):
        pred_set = []
        for k in range(probs.shape[^1]):
            if 1 - probs[i, k] <= threshold:
                pred_set.append(k)
        
        if not pred_set:  # Empty set - include most likely class
            pred_set = [np.argmax(probs[i])]
            
        prediction_sets.append(pred_set)
        set_sizes.append(len(pred_set))
    
    conformal_info = {
        "threshold": threshold,
        "average_set_size": np.mean(set_sizes),
        "coverage_level": 1 - alpha,
        "quantile_level": quantile_level
    }
    
    return prediction_sets, conformal_info


def venn_abers(probs: np.ndarray, labels: np.ndarray,
               val_probs: np.ndarray, val_labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """Venn-Abers calibration (simplified binary version)"""
    # Simplified implementation for multi-class using one-vs-rest
    calibrated_probs = probs.copy()
    
    for k in range(probs.shape[^1]):
        # Binary problem: class k vs rest
        binary_labels = (labels == k).astype(int)
        val_binary_labels = (val_labels == k).astype(int)
        
        class_probs = probs[:, k]
        val_class_probs = val_probs[:, k]
        
        # Use isotonic regression as approximation
        iso_reg = IsotonicRegression(out_of_bounds='clip')
        iso_reg.fit(val_class_probs, val_binary_labels)
        
        calibrated_probs[:, k] = iso_reg.predict(class_probs)
    
    # Renormalize to proper probability distribution
    calibrated_probs = calibrated_probs / np.sum(calibrated_probs, axis=1, keepdims=True)
    
    # Return probabilities and prediction intervals (placeholder)
    intervals = np.zeros((len(probs), 2))  # Placeholder
    
    return calibrated_probs, intervals


# =============================================================================
# UNCERTAINTY QUANTIFICATION METHODS (STUBS)
# =============================================================================

def deep_ensemble_stub(probs_list: List[np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:
    """Deep ensemble uncertainty (stub implementation)"""
    return prompt_ensemble(probs_list).calibrated_probs, np.var(probs_list, axis=0).mean(axis=1)


def mc_dropout_stub(probs: np.ndarray, n_samples: int = 10) -> Tuple[np.ndarray, np.ndarray]:
    """MC Dropout uncertainty (stub)"""
    # Simulate multiple forward passes with dropout
    dropout_samples = []
    for _ in range(n_samples):
        noise = np.random.normal(0, 0.05, probs.shape)
        dropout_probs = np.clip(probs + noise, 0, 1)
        dropout_probs = dropout_probs / dropout_probs.sum(axis=1, keepdims=True)
        dropout_samples.append(dropout_probs)
    
    mean_probs = np.mean(dropout_samples, axis=0)
    uncertainty = np.var(dropout_samples, axis=0).mean(axis=1)
    
    return mean_probs, uncertainty


def swag_stub(probs: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """SWAG uncertainty (stub)"""
    # Placeholder - would require actual SWAG implementation
<span style="display:none">[^2][^3][^4][^5][^6][^7][^8]</span>

<div style="text-align: center">⁂</div>

[^1]: https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation
[^2]: https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/evaluation/list-of-eval-metrics
[^3]: https://www.evidentlyai.com/llm-guide/llm-evaluation-metrics
[^4]: https://www.dezlearn.com/llm-evaluation-metrics/
[^5]: https://arya.ai/blog/llm-evaluation-metrics
[^6]: https://wandb.ai/onlineinference/genai-research/reports/LLM-evaluation-Metrics-frameworks-and-best-practices--VmlldzoxMTMxNjQ4NA
[^7]: https://community.sap.com/t5/technology-blog-posts-by-sap/evaluating-llm-powered-systems/ba-p/14102433
[^8]: https://machinelearningmastery.com/llm-evaluation-metrics-made-easy/```

