

# now giv eme more indepth detailed theory of visualization based techniques

Part 2: Visual Based Criteria (Expanded Theory for Visualization Metrics)

This part expands on the visualization criteria for evaluating LLM confidence in email classification, providing more in-depth detailed theory for each. Each criterion includes an extended theoretical background (origin, intuition, detailed links to statistics/decision theory, information theory, Bayesian inference, or reliability theory, with references from search results), formula with variable-by-variable explanations, interpretation (what good/bad values mean), reason to choose, when to choose (scenarios like dataset type, imbalance, safety-critical, drift, OOD, noisy labels, dashboards), advantages, and disadvantages. These techniques are essential for LLM confidence assessment, as they transform abstract metrics into intuitive visuals, revealing patterns like overconfidence in Spam or uncertainty in rare Forums classes. Search results emphasize their use in LLM calibration (e.g., reliability diagrams for overconfidence ), uncertainty quantification (e.g., histograms for score clustering ), and selective prediction (e.g., curves for confidence-accuracy ). In email workflows, they help visualize risks like false negatives in phishing detection.[^1][^2][^4][^6][^7]

4.16 Reliability Diagrams (Overall, Per-class, Adaptive Bins)

Detailed Theoretical Background: Reliability diagrams, originating from probabilistic forecasting in meteorology (Murphy, 1973), plot predicted confidence against observed accuracy to assess calibration quality. The intuition is to visually diagnose if a model's confidence aligns with empirical reality—perfect calibration forms the identity line y=x, where confidence equals accuracy. They connect to reliability theory in statistics, where deviations indicate systematic biases (e.g., overconfidence below the line). In information theory, they relate to cross-entropy minimization by showing where probability distributions misalign with true outcomes. Bayesian inference views them as posterior checks, visualizing if predicted probabilities match empirical frequencies. Adaptive bins (equal sample sizes) address fixed binning bias, drawing from kernel density estimation for smoother curves (Nixon et al., 2019). Per-class variants use one-vs-rest for multi-class, enabling detection of class-specific miscalibration. In LLM classification, they reveal overconfidence in transformers due to softmax sharpness (Guo et al., 2017), as seen in email tasks where Spam might show below-diagonal curves from high-frequency data leading to unwarranted certainty. They also link to decision theory for threshold optimization, helping in safety-critical apps by highlighting unreliable confidence regions.[^4][^7][^1]

Formula with variable-by-variable explanations:

For each bin B_m in M bins:
x_m = conf(B_m) = (1/|B_m|) Σ_{i in B_m} max_k p̂_{i,k} (average top-label confidence)
y_m = acc(B_m) = (1/|B_m|) Σ_{i in B_m} 1[ŷ_i = y_i] (empirical accuracy fraction)
Plot points (x_m, y_m) with reference line y = x

Adaptive binning: Sort samples by confidence, divide into M bins with equal |B_m| ≈ N/M
Per-class: For class k, x_m = avg p̂_{i,k} in bin, y_m = fraction where y_i = k

- B_m: m-th bin (partition of confidence range  or adaptive by sample count)[^1]
- conf(B_m): Mean predicted confidence in bin m (x-axis, model's self-reported certainty)
- acc(B_m): Mean observed accuracy in bin m (y-axis, empirical correctness rate)
- |B_m|: Number of samples in bin m (weight for averaging)
- max_k p̂_{i,k}: Top-label probability for sample i (focus on predicted class)
- 1[ŷ_i = y_i]: Binary indicator of correctness (1 if predicted matches true)
- y = x: Ideal calibration line (theory baseline for perfect alignment)

Interpretation: Points on y=x indicate perfect calibration (good, confidence = accuracy); points below show overconfidence (bad, confidence > accuracy, risking false positives); points above underconfidence (bad, too conservative, missing opportunities). Adaptive bins yield smoother curves; large deviations in per-class diagrams signal class-specific issues. Good diagrams have points hugging the diagonal with small variance; bad ones show systematic shifts, indicating untrustworthy probabilities.

Reason to choose: Most intuitive calibration visualization; reveals systematic patterns; easy to interpret for stakeholders; standard in LLM literature for overconfidence diagnosis.[^1]

When to choose: Primary calibration assessment visualization; explaining calibration to non-technical stakeholders; identifying systematic calibration patterns; comparing calibration across models or time periods; dataset imbalance for per-class diagrams to spot rare class overconfidence; safety-critical to visualize worst-case overconfidence (e.g., Spam FNs); drift monitoring by comparing diagrams over time; OOD detection if new data deviates more from diagonal; noisy labels to see if confidence aligns with accuracy despite noise; dashboards for visual calibration tracking and quick diagnostics.

Advantages: Highly intuitive and interpretable (direct visual of confidence-accuracy gap); reveals systematic patterns (e.g., overconfidence at high levels); standard visualization across literature; easy to create and customize (adaptive for robustness, per-class for multi-class); supports quantitative links to ECE; helps in decision theory by showing threshold reliability.

Disadvantages: Sensitive to binning strategy (number/width affects smoothness, adaptive mitigates but loses uniform intervals); can be noisy with insufficient data per bin; may not show fine-grained issues (within-bin variance hidden); requires careful bin size selection to balance resolution and reliability; static and doesn't capture sample distributions or temporal dynamics; per-class variants increase complexity for large K.

4.17 Boxplots (Agreement vs Disagreement)

Detailed Theoretical Background: Boxplots, developed by John Tukey (1977) for exploratory data analysis, compare distributions of confidence/uncertainty measures across subgroups (e.g., agreement=1 vs 0). The intuition is to summarize central tendency, spread, and outliers to validate if auxiliary signals like agreement correlate with uncertainty. They connect to statistical comparison in decision theory for subgroup fairness and Bayesian inference for conditional distributions (e.g., posterior uncertainty given agreement). In LLMs, they extend metamorphic testing by visualizing if disagreed (ambiguous) samples have higher variance/lower median confidence, as in data labeling confidence estimation. For email classification, they reveal if low-agreement emails (e.g., borderline Spam) have appropriately low confidence, linking to reliability theory for conditional calibration and information theory for entropy spread. They also tie to OOD detection by showing distribution shifts in uncertain subgroups.[^2][^5][^8]

Formula with variable-by-variable explanations (No direct formula; statistical summaries):

For groups G (agreement=1/0):
Box: Median, Q1 (25th percentile), Q3 (75th percentile) of confidence scores
Whiskers: Typically Q1 - 1.5*IQR to Q3 + 1.5*IQR (interquartile range = Q3 - Q1)
Outliers: Points beyond whiskers

Test: Mann-Whitney U = Σ ranks for group differences; effect size Cohen's d = (μ1 - μ0) / σ_pooled

- Median: Central tendency of confidence in group (robust mean-like measure)
- Q1, Q3: Lower/upper quartiles (box edges, capturing 50% data spread)
- IQR = Q3 - Q1: Measure of variability (statistical dispersion)
- Whiskers: Range of non-outlier data (extended to 1.5*IQR for standard boxplot)
- Outliers: Extreme values (potential anomalies or high-uncertainty cases)
- U: Test statistic for distribution shift (non-parametric rank sum)
- Cohen's d: Standardized mean difference (effect size for practical significance)
- μ1, μ0: Means for agreement=1/0 groups
- σ_pooled: Pooled standard deviation (combined variance)

Interpretation: Well-separated boxes (e.g., lower median/IQR for disagreement) indicate good uncertainty capture (good, confidence reflects ambiguity); overlapping boxes suggest poor differentiation (bad, model ignores auxiliary signals). Many outliers in disagreement group signal high variance (bad if uncalibrated). Good boxplots show tight, high medians for easy groups; bad ones show wide overlap, indicating unreliable confidence.

Reason to choose: Validates auxiliary signals; reveals subgroup differences; provides statistical evidence for contextual calibration; intuitive for distribution comparison.

When to choose: Validating agreement labels or other auxiliary signals; understanding uncertainty patterns across subgroups; justifying contextual calibration approaches; diagnostic analysis of model behavior; dataset imbalance to compare rare class distributions vs common; safety-critical for subgroup fairness (e.g., ensure no bias in ambiguous emails); drift detection in subgroup shifts over time; OOD by comparing to in-distribution groups; noisy labels to see confidence spread despite noise; dashboards for quick subgroup monitoring and statistical tests.

Advantages: Clear visual comparison of distributions (median, spread, outliers); statistical tests (e.g., Mann-Whitney) provide quantitative validation; reveals subgroup patterns invisible in aggregate metrics; supports contextual calibration decisions; handles non-normal distributions well; intuitive for identifying outliers and spread; low computation cost for quick diagnostics.

Disadvantages: Limited to comparing two or few groups (becomes cluttered otherwise); may not capture complex multivariate relationships (e.g., interactions with other features); requires sufficient samples in each group for reliable quartiles; static visualization doesn't show temporal or causal dynamics; sensitive to outliers (though it highlights them); doesn't quantify overlap without additional tests like Kolmogorov-Smirnov.

4.18 Heatmaps (Score–Correctness Correlations)

Detailed Theoretical Background: Heatmaps visualize correlation matrices between confidence scores, correctness, and features, originating from multivariate analysis in statistics (e.g., Pearson, 1895) and heatmap visualization in bioinformatics. The intuition is to color-code pairwise relationships to reveal redundancies, complementary information, or unexpected patterns in uncertainty measures. They connect to information theory via mutual information (approximated by correlations) and Bayesian networks for dependency modeling. In LLMs, heatmaps extend uncertainty diagnostics by showing if confidence correlates negatively with error (good calibration) or if scores are redundant (e.g., margin vs entropy). For email classification, they highlight if agreement correlates with correctness, aiding in bias detection per decision theory, and link to OOD by showing correlation drops in novel data. They also tie to reliability theory for score validation.[^6][^8]

Formula with variable-by-variable explanations:

R_{ij} = corr(score_i, score_j) = cov(score_i, score_j) / (σ_i * σ_j)
Heatmap: Color intensity proportional to R_{ij} (e.g., red positive, blue negative, scale -1 to 1)

- R_{ij}: Pearson correlation coefficient between scores i and j (linear dependence measure)
- cov(score_i, score_j): Covariance (joint variability)
- σ_i, σ_j: Standard deviations (normalization for scale-invariance)
- Color intensity: Visual mapping (e.g., darker for |R| > 0.5, with sign for direction)
- Scores: Include confidence, entropy, margin, correctness (1/0), agreement, etc.

Interpretation: |R| ≈ 1 high correlation (good for consistent scores, bad if redundant); |R| ≈ 0 uncorrelated (good for complementary measures); negative R with error = good (high confidence low error). Good heatmaps show expected patterns (e.g., high negative entropy-error); bad ones show weak or unexpected correlations, indicating poor uncertainty capture.

Reason to choose: Reveals relationships between uncertainty measures; guides selection of complementary metrics; identifies redundancies; compact multivariate visualization.

When to choose: Selecting diverse uncertainty measures for ensemble methods; understanding relationships between different confidence scores; identifying redundant measurements; exploratory analysis of uncertainty structure; dataset imbalance to check per-class correlations (e.g., low in rare Forums); safety-critical for validating score consistency across features; drift detection in changing correlations over time; OOD by correlation drops with new data; noisy labels for robustness checks (e.g., if correctness correlation holds); dashboards for relationship overviews and pattern spotting.

Advantages: Comprehensive view of pairwise relationships in one plot; helps select non-redundant measures for efficiency; reveals unexpected correlations (e.g., agreement-confidence); compact for many variables; intuitive color coding for quick insights; supports statistical significance overlays; low cost for correlation computation.

Disadvantages: Limited to pairwise linear relationships (misses non-linear or higher-order dependencies); may become cluttered with many variables (needs clustering); doesn't show causal relationships (only associations); sensitive to outliers or non-normal data (use Spearman for robustness); requires normalization for fair comparison; static and doesn't show temporal changes.

4.19 Confidence Histograms, Violin Plots Per Class

Detailed Theoretical Background: Histograms estimate probability density of confidence scores per class, originating from frequency distributions in statistics (Pearson, 1895). Violin plots combine histograms with kernel density estimation (KDE) and boxplots for richer visualization (Hintze \& Nelson, 1998). The intuition is to show distribution shape, central tendency, and variability to identify class-specific patterns (e.g., bimodal confidence indicating uncertainty). They connect to information theory via entropy estimation from density (high entropy = wide histogram) and Bayesian inference for per-class posteriors. In LLMs, they reveal overconfidence (narrow peaks at high values) or underconfidence, as in data labeling where low-confidence clusters correspond to errors. For email classification, they visualize imbalance effects (e.g., broader violins for rare Forums due to low data) and link to decision theory for class-wise risk assessment. KDE in violins provides continuous density, improving on discrete histograms.[^5][^7][^2]

Formula with variable-by-variable explanations (Histogram):

Hist_k(b) = count of confidence scores in bin b for class k / (N_k * bin_width) (density)
Violin: KDE(conf_k) + boxplot (median, IQR), where KDE = (1/N h) Σ K((x - x_i)/h) (Gaussian kernel K)

- Hist_k(b): Density in bin b for class k (frequency normalized)
- N_k: Samples in class k
- bin_width: Bin size (affects resolution)
- KDE: Kernel density estimate (smooth curve)
- h: Bandwidth (smoothing parameter)
- K: Kernel function (e.g., Gaussian for normal approximation)
- Boxplot: Embedded summary (median line, IQR box, whiskers)

Interpretation: Narrow histogram/violin with high mode near 1 = sharp, confident predictions (good if calibrated); wide or low mode = uncertain, hedged (bad if overconfident). Skewed violins indicate bias (e.g., left-skew bad for low confidence on correct). Good plots show consistent shapes across classes; bad ones show class-specific anomalies like wide Forums from imbalance.

Reason to choose: Reveals class-specific confidence patterns; identifies systematic biases; combines density with summaries for comprehensive view.

When to choose: Imbalanced datasets with different class characteristics; understanding confidence patterns across different classes; diagnosing class-specific calibration issues; multi-class problems with varying class difficulty; production monitoring requiring per-class insights; safety-critical to spot low-confidence classes (e.g., Forums risks); drift detection if distributions widen; OOD where new class distributions differ; noisy labels to see if confidence spreads increase; dashboards for distributional overviews.

Advantages: Clear visualization of per-class patterns (shape, spread, modes); reveals systematic class-specific biases (e.g., overconfidence peaks); combines distributional shape with summary statistics (violin advantage); guides targeted calibration improvements; handles small classes with KDE smoothing; intuitive for non-experts.

Disadvantages: Can become cluttered with many classes (needs faceting); requires sufficient samples per class for reliable estimates (rare classes noisy); histograms sensitive to bin size; violins computationally heavier with KDE; static view doesn't show temporal patterns; may mask multimodal distributions if not zoomed.

4.20 Confidence–Error Curves

Detailed Theoretical Background: Plots error rate as a function of confidence threshold, originating from threshold analysis in decision theory and cumulative distribution functions in statistics. The intuition is to show how prediction quality improves with stricter confidence requirements, linking to selective prediction where low-confidence abstention reduces risk (El-Yaniv \& Wiener, 2010). They connect to information theory via conditional entropy (error given confidence) and Bayesian decision theory for optimal thresholds. In LLMs, they visualize if confidence correlates with correctness (steep decrease = good), often used in calibration to check post-hoc improvements. For email, they reveal if high-confidence Spam classifications have low error, aiding in false negative risk assessment. They also tie to OOD by showing flatter curves for unfamiliar data.[^7][^4][^1]

Formula with variable-by-variable explanations:

Error(c) = P(incorrect | confidence ≥ c) = (\# errors in samples with conf ≥ c) / (\# samples with conf ≥ c)
Curve: Plot c (x-axis, from 0 to 1) vs Error(c) (y-axis)

- c: Confidence threshold (varying from low to high)
- P(incorrect | confidence ≥ c): Conditional probability of error above threshold (error rate in selected subset)
- 
# errors: Count of wrong predictions in subset

- 
# samples: Size of subset above c (coverage decreases as c increases)


Interpretation: Steep monotonic decrease = good (high confidence = low error, reliable); flat or non-monotonic = bad (confidence doesn't predict correctness). Good curves drop to near 0 error at high c; bad ones remain high, indicating poor uncertainty.

Reason to choose: Direct visualization of confidence quality; reveals confidence-accuracy relationship; guides threshold setting for selective prediction.

When to use: Evaluating confidence estimation quality; setting confidence thresholds for selective prediction; comparing different confidence measures; understanding confidence-accuracy relationships; safety-critical to ensure low error at high confidence; imbalance to check if rare classes have flatter curves; drift if curve flattens over time; OOD where curve doesn't decrease; noisy labels to see robustness; dashboards for threshold optimization.

Advantages: Direct assessment of confidence utility (predicts error); provides actionable insights for threshold setting (e.g., find c for target error); intuitive interpretation (steeper better); useful for selective prediction systems; complements numerical metrics like AURC; easy to plot and compare methods.

Disadvantages: Requires sufficient data across confidence levels (noisy at extremes with few samples); may be sensitive to class imbalance (weighted error needed); doesn't account for different error costs; single curve may not capture complex patterns (e.g., multi-modal confidence); static and ignores per-class variations unless stratified.

4.21 Temperature Sweeps

Detailed Theoretical Background: Temperature sweeps plot metrics (e.g., NLL, ECE) against varying temperature parameters, rooted in entropy regularization theory and softmax temperature in neural networks (Hinton et al., 2015). The intuition is to find optimal T that minimizes miscalibration, with T>1 "cooling" overconfident distributions. They connect to information theory via entropy maximization (high T increases entropy) and Bayesian inference for temperature as prior strength. In LLMs, sweeps diagnose systematic overconfidence (U-shaped NLL curve with min T>1) and guide calibration, as in GPT classification where sweeps improve confidence estimates. For email, they visualize if higher T is needed for rare classes to temper overconfidence.[^4][^7][^1]

Formula with variable-by-variable explanations:

Metric(T) = f(softmax(z / T)) for T in range (e.g., 0.1 to 5.0)
e.g., NLL(T) = -Σ log p_T(y|x)

- T: Temperature parameter (x-axis, controls sharpness: T>1 flattens, T<1 sharpens)
- z: Logits (pre-softmax scores)
- softmax(z / T): Temperature-scaled probabilities (softens for T>1)
- f: Evaluation metric (y-axis, e.g., NLL or ECE)
- Range: Sweep values to find minimum (optimal T)

Interpretation: U-shaped curve with clear minimum = good (optimal T exists, model responsive to scaling); flat curve = bad (insensitive to temperature, deep miscalibration). Good sweeps show min T~1-2 for overconfidence; bad ones have no minimum or min at extreme T.

Reason to choose: Guides temperature scaling parameter selection; reveals systematic confidence biases; shows calibration sensitivity.

When to use: Temperature scaling parameter selection; understanding systematic overconfidence patterns; monitoring calibration drift over time; comparing models' confidence calibration needs; imbalance to see per-class optimal T; safety-critical for ensuring scalable confidence; OOD if sweep minimum shifts; noisy labels to find robust T; dashboards for interactive tuning.

Advantages: Direct guidance for temperature parameter (finds optimal via visualization); reveals systematic biases (e.g., T>1 for overconfidence); shows sensitivity to calibration correction; useful for monitoring and debugging; connects to theory for entropy analysis; easy to generate with varying T.

Disadvantages: Limited to temperature scaling method (not general); may not reveal complex calibration patterns (assumes uniform scaling); requires validation set for proper temperature selection; single metric per sweep limits multi-objective view; computational cost for full sweep; assumes monotonic response to T.

4.22 Risk–Coverage Curves

Detailed Theoretical Background: Risk-coverage curves plot error rate (risk) against fraction of predictions made (coverage), originating from selective prediction theory (El-Yaniv \& Wiener, 2010) and cost-sensitive learning. The intuition is to visualize the trade-off between making more predictions (high coverage, high risk) and abstaining on uncertain ones (low coverage, low risk). They connect to decision theory for optimal abstention thresholds and information theory via conditional risk. In LLMs, they assess if confidence enables effective selective prediction, as in data labeling where low-confidence rejection improves quality. For email, they show if abstaining on low-confidence (e.g., ambiguous Forums) reduces overall risk. AURC is the area under this curve, linking to integral risk measures.[^5][^7]

Formula with variable-by-variable explanations:

Risk(τ) = error rate in top τ fraction of confident predictions (y-axis)
Coverage τ = fraction of samples predicted (x-axis, 0 to 1)

Curve: Sort by confidence descending, compute cumulative error for increasing τ

- τ: Coverage level (x-axis, 1 = all predicted, 0 = none)
- Risk(τ): Conditional error rate at coverage τ (y-axis, ideally decreases with lower τ)
- Top τ fraction: Samples with highest confidence (selective subset)
- Error rate: Fraction incorrect in subset (risk measure)

Interpretation: Steep decrease to low risk at low τ = good (confidence identifies reliable predictions); flat or high risk = bad (poor uncertainty). Good curves approach 0 risk quickly; bad ones stay high, indicating unreliable abstention.

Reason to choose: Essential for selective prediction evaluation; visualizes confidence quality; guides coverage threshold selection.

When to use: Selective prediction system design; confidence estimation method comparison; setting coverage thresholds for production; human-in-the-loop system optimization; imbalance to check rare class risk at low coverage; safety-critical for low-risk high-confidence subsets; drift if curve flattens; OOD where risk doesn't decrease; noisy labels for robust selection; dashboards for trade-off analysis.

Advantages: Direct visualization of selective prediction trade-offs; provides actionable threshold information (e.g., risk at 80% coverage); integrates confidence quality assessment; essential for abstention-capable systems; complements AURC for visual insight; easy to compare methods.

Disadvantages: Requires good confidence estimates to be meaningful; may not account for different error costs (needs weighted variant); single curve may not capture per-class trade-offs; interpretation requires domain expertise for threshold selection; sensitive to sorting quality; computational for large N.

4.23 ROC/PR Overlays

Detailed Theoretical Background: ROC (Receiver Operating Characteristic) and PR (Precision-Recall) curves overlay performance for discrimination, originating from signal detection theory (Green \& Swets, 1966) and information retrieval. ROC plots TPR vs FPR; PR plots precision vs recall. Overlays compare methods or classes. They connect to decision theory for threshold selection and Bayesian inference for likelihood ratios. In LLMs, overlays visualize if calibration preserves discrimination (ranking invariant). For email, they show Spam detection trade-offs (high recall for safety), with macro/micro for imbalance. AUROC/AUPRC are areas under these curves.[^6][^7][^1]

Formula with variable-by-variable explanations:

ROC: Plot TPR = TP/(TP+FN) (y) vs FPR = FP/(FP+TN) (x) at varying thresholds
PR: Plot Precision = TP/(TP+FP) (y) vs Recall = TP/(TP+FN) (x)
Overlay: Multiple curves on same plot (e.g., per-method or per-class)

- TPR/Recall: Sensitivity (true positive fraction)
- FPR: 1 - specificity (false positive fraction)
- Precision: Positive predictive value
- Threshold: Varying confidence cutoff for binary decisions
- Overlay: Superimposed lines for comparison (e.g., dashed for raw, solid for calibrated)

Interpretation: Curves close to top-left (ROC) or top-right (PR) = good discrimination; area >0.8 good, ~0.5 random. Good overlays show similar shapes (calibration preserves ranking); bad show degradation.

Reason to choose: Standard discriminative performance visualization; enables model comparison; reveals class-specific patterns.

When to use: Comparing discriminative performance across models; understanding per-class performance in multi-class settings; visualizing performance before/after calibration; selecting operating thresholds for deployment; imbalance (PR better); safety-critical for high-recall thresholds (e.g., Spam); drift if curves degrade; OOD where PR drops; noisy labels for robust AUC; dashboards for performance overlays.

Advantages: Standard, well-understood visualizations; enable direct model comparison; reveal class-specific performance patterns; show threshold-performance trade-offs; AUROC/AUPRC provide quantitative summaries; useful for multi-class with macro/micro.

Disadvantages: Don't show calibration quality (only discrimination); can become cluttered with many classes/models; may not reflect actual deployment operating points (full curve not always used); AUROC misleading with severe imbalance (use PR); require binary framing for multi-class; static and ignore confidence distributions.

4.24 Cumulative Gain / Lift Charts

Detailed Theoretical Background: Cumulative gain and lift charts show the benefit of model-based ranking over random, originating from marketing (e.g., response modeling) and information retrieval (e.g., precision at k). Gain plots cumulative positives captured in top k%; lift is gain over baseline. They connect to decision theory for prioritization and Bayesian ranking for expected utility. In LLMs, they visualize confidence for ranking tasks, like prioritizing high-confidence emails. For email classification, they assess if high-confidence Spam is captured early, linking to cost-sensitive theory where lift quantifies ROI of classification.[^7][^5]

Formula with variable-by-variable explanations:

Gain(k) = (\# true positives in top k% ranked by confidence) / total true positives (y-axis)
Lift(k) = Gain(k) / (k/100) = precision in top k% / overall precision
Plot k% (x-axis, 0 to 100) vs Gain/Lift (y-axis)

- k: Percentage of ranked samples (x-axis, cumulative fraction)
- 
# true positives in top k%: Count of correct in highest-confidence k%

- Total true positives: Overall correct (normalization for gain)
- Precision in top k%: Positives rate in subset
- Overall precision: Baseline rate (for lift ratio)

Interpretation: Steep gain to 1 at low k = good (model ranks positives high); lift >1 good (better than random). Good charts show high lift early; bad ones follow diagonal (no benefit).

Reason to choose: Shows practical value of confidence for prioritization; relevant for cost-sensitive applications; intuitive business interpretation.

When to use: Demonstrating business value of predictions; optimizing resource allocation (e.g., manual review capacity); marketing and customer targeting applications; prioritization systems with limited processing capacity; imbalance to see gain in rare classes; safety-critical for ranking high-risk emails; drift if gain flattens; OOD where lift drops; noisy labels for robust ranking; dashboards for ROI visualization.

Advantages: Clear business value interpretation (e.g., lift as multiplier); shows practical benefit of using model vs random selection; intuitive for non-technical stakeholders; directly relevant for prioritization applications; supports custom positives (e.g., Spam only); quantifies cumulative advantage.

Disadvantages: Primarily relevant for ranking/prioritization tasks (not pure classification); may not be appropriate for all scenarios (assumes positives to gain); requires understanding of business context for interpretation; less standard than ROC/PR in ML literature; sensitive to ranking quality; static and ignores per-class gains unless stratified.
<span style="display:none">[^3]</span>

<div style="text-align: center">⁂</div>

[^1]: https://latitude-blog.ghost.io/blog/5-methods-for-calibrating-llm-confidence-scores/

[^2]: https://www.mindee.com/blog/how-use-confidence-scores-ml-models

[^3]: https://arxiv.org/html/2406.03441v1

[^4]: https://www.nyckel.com/blog/calibrating-gpt-classifications/

[^5]: https://refuel.ai/blog-posts/labeling-with-confidence

[^6]: https://arxiv.org/html/2410.13047v1

[^7]: https://d30i16bbj53pdg.cloudfront.net/wp-content/uploads/2024/08/Decoding-Unstructured-Text-Enhancing-LLM-Classification-Accuracy-with-Redundancy-and-Confidence.pdf

[^8]: https://arxiv.org/abs/2502.07186

